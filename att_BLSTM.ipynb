{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention-Based Bidirectional Long Short-Term Memory Networks for Relation Classification   ACL  2016"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1前言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1,1课程回顾"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='imgs/overall.png' width=\"800\" height=\"800\" align=\"bottom\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 模型结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/model.png\"  width=\"600\" height=\"600\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 代码结构展示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/dir.png\"  width=\"300\" height=\"300\" align=\"bottom\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 准备工作\n",
    "### 2.1项目环境配置\n",
    "\n",
    "* Python3.8\n",
    "* jupyter notebook\n",
    "* torch            1.6.0+cu10.2\n",
    "* numpy            1.18.5\n",
    "* ConfigArgParse      1.5.2\n",
    "* torchtext         0.7.0\n",
    "\n",
    "代码运行环境建议使用Visual Studio Code(VScode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 数据集下载\n",
    "* 执行文件 `python reader.py`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 项目代码结构（VScode中演示）\n",
    "\n",
    ">1）是什么？\n",
    "\n",
    "　　我们首先会在VScode环境中让代码跑一下，直观感受到项目的训练，并展示前向推断的输出，让大家看到模型的效果。\n",
    ">2）怎么构成的？\n",
    "\n",
    "　　然后介绍项目代码的构成，介绍项目有哪些文件夹，包含哪些文件，这些文件构成了什么功能模块如：数据预处理模块，模型设计模块，损失函数模块，推断与评估模块。\n",
    ">3）小结\n",
    "\n",
    "　　在主文件中在过一下启动训练的流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 算法模块及细节（jupyter和VScode中演示）\n",
    "\n",
    "　　在jupyter notebook中细致地讲解每一个模块。\n",
    "  \n",
    "　　以实现模块功能为目的，来讲解每个函数的执行流程，呈现中间数据，方便同学们理解学习。\n",
    "  \n",
    "　　内容分为以下几个模块：**超参数设置，数据读取与处理，模型定义，模型训练，模型评价**。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 超参数设置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import random\n",
    "import configargparse\n",
    "from utils import show_time, fwrite, shell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args():\n",
    "    cur_time = show_time(printout=False)\n",
    "    parser = configargparse.ArgumentParser(\n",
    "        description='Args for Text Classification')\n",
    "    group = parser.add_argument_group('Model Hyperparameters')\n",
    "    group.add_argument('-f')\n",
    "    group.add_argument('-init_xavier', default=False, action='store_true',\n",
    "                       help='whether to use xavier normal as initiator for model weights')\n",
    "    group.add_argument('-emb_dropout', default=0.3, type=float,\n",
    "                       help='dropout of the embedding layer')\n",
    "    group.add_argument('-emb_dim', default=100, type=int,\n",
    "                       help='dimension of embedding vectors')\n",
    "    group.add_argument('-vocab_max_size', default=100000, type=int,\n",
    "                       help='max number of words in vocab')\n",
    "    group.add_argument('-lstm_n_layer', default=1, type=int,\n",
    "                       help='num of layers in LSTM')\n",
    "    group.add_argument('-lstm_dropout', default=0.3, type=float,\n",
    "                       help='dropout in >=1th LSTM layer')\n",
    "    group.add_argument('-lstm_dim', default=100, type=int,\n",
    "                       help='dimension of the lstm hidden states')\n",
    "    group.add_argument('-lstm_combine', default='add',\n",
    "                       choices=['add', 'concat'], type=str,\n",
    "                       help='the way to combine bidirectional lstm outputs')\n",
    "    group.add_argument('-n_linear', default=1, type=int,\n",
    "                       help='number of linear layers after lstm')\n",
    "    group.add_argument('-linear_dropout', default=0.5, type=float,\n",
    "                       help='dropout of the penultimate layer')\n",
    "    group.add_argument('-n_classes', default=2, type=int,\n",
    "                       help='number of classes to predict')\n",
    "\n",
    "    group = parser.add_argument_group('Training Specs')\n",
    "    group.add_argument('-seed', default=0, type=int, help='random seed')\n",
    "    group.add_argument('-batch_size', default=10, type=int, help='batch size')\n",
    "    group.add_argument('-epochs', default=100, type=int,\n",
    "                       help='number of epochs to train the model')\n",
    "    group.add_argument('-lr', default=1.0, type=float, help='learning rate')\n",
    "    group.add_argument('-weight_decay', default=1e-5, type=float,\n",
    "                       help='weight decay')\n",
    "\n",
    "    group = parser.add_argument_group('Files')\n",
    "    group.add_argument('-data_dir', default='data/re_semeval/', type=str,\n",
    "                       help='the directory for data files')\n",
    "    group.add_argument('-train_fname', default='train.csv', type=str,\n",
    "                       help='training file name')\n",
    "    group.add_argument('-data_sizes', nargs=3, default=[None, None, None],\n",
    "                       type=int,\n",
    "                       help='# samples to use in train/dev/test files')\n",
    "    group.add_argument('-preprocessed', action='store_false', default=True,\n",
    "                       help='whether input data is preprocessed by spacy')\n",
    "    group.add_argument('-lower', action='store_true', default=False,\n",
    "                       help='whether to lowercase the input data')\n",
    "\n",
    "    group.add_argument('-uid', default=cur_time, type=str,\n",
    "                       help='the id of this run')\n",
    "    group.add_argument('-save_dir', default='tmp/', type=str,\n",
    "                       help='directory to save output files')\n",
    "    group.add_argument('-save_dir_cp', default='tmp_cp/', type=str,\n",
    "                       help='directory to backup output files')\n",
    "    group.add_argument('-save_meta_fname', default='run_meta.txt', type=str,\n",
    "                       help='file name to save arguments and model structure')\n",
    "    group.add_argument('-save_log_fname', default='run_log.txt', type=str,\n",
    "                       help='file name to save training logs')\n",
    "    group.add_argument('-save_valid_fname', default='valid_e00.txt', type=str,\n",
    "                       help='file name to save valid outputs')\n",
    "    group.add_argument('-save_vis_fname', default='example.txt', type=str,\n",
    "                       help='file name to save visualization outputs')\n",
    "    group.add_argument('-save_model_fname', default='model', type=str,\n",
    "                       help='file to torch.save(model)')\n",
    "    group.add_argument('-save_vocab_fname', default='vocab.json', type=str,\n",
    "                       help='file name to save vocab')\n",
    "\n",
    "    group = parser.add_argument_group('Run specs')\n",
    "    group.add_argument('-n_gpus', default=1, type=int, help='# gpus to run on')\n",
    "    group.add_argument('-load_model', default='', type=str,\n",
    "                       help='path to pretrained model')\n",
    "    group.add_argument('-verbose', action='store_true', default=False,\n",
    "                       help='whether to show pdb.set_trace() or not')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data(save_dir='./tmp', data_dir='./data/wiki_person',\n",
    "                train_fname='train.csv', data_sizes=[None, None, None],\n",
    "                skip_header=True, verbose=True):\n",
    "    files = ['train', 'valid', 'test']\n",
    "    suffix = '.' + train_fname.split('.')[-1]\n",
    "    n_lines = {}\n",
    "\n",
    "    def _get_num_lines(file):\n",
    "        with open(file) as f:\n",
    "            data = [line.strip() for line in f if line]\n",
    "        num_lines = len(data) if not skip_header else len(data) - 1\n",
    "        return num_lines\n",
    "\n",
    "    for file, data_size in zip(files, data_sizes):\n",
    "\n",
    "        read_from = os.path.join(data_dir,\n",
    "                                 train_fname.replace('train', file))\n",
    "        save_to = os.path.join(save_dir, file + suffix)\n",
    "\n",
    "        with open(read_from) as f:\n",
    "            data = [line for line in f]\n",
    "        if skip_header:\n",
    "            header, body = data[:1], data[1:]\n",
    "        else:\n",
    "            header, body = [], data\n",
    "        random.shuffle(body)\n",
    "        data = header + body[:data_size]\n",
    "\n",
    "        fwrite(''.join(data), save_to)\n",
    "\n",
    "        n_lines[file] = _get_num_lines(save_to)\n",
    "\n",
    "    if verbose:\n",
    "        writeout = ['{}: {}'.format(*item) for item in n_lines.items()]\n",
    "        writeout = ', '.join(writeout)\n",
    "        print('[Info] #samples in', writeout)\n",
    "    return list(n_lines.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup():\n",
    "    args = get_args()\n",
    "\n",
    "    if not os.path.isdir(args.save_dir):\n",
    "        os.mkdir(args.save_dir)\n",
    "    elif not args.load_model:\n",
    "        shell('rm {}/*'.format(args.save_dir))\n",
    "    args.save_meta_fname = os.path.join(args.save_dir, args.save_meta_fname)\n",
    "    args.save_log_fname = os.path.join(args.save_dir, args.save_log_fname)\n",
    "    args.save_valid_fname = os.path.join(args.save_dir, args.save_valid_fname)\n",
    "    args.save_vis_fname = os.path.join(args.save_dir, args.save_vis_fname)\n",
    "    args.save_model_fname = os.path.join(args.save_dir, args.save_model_fname)\n",
    "    args.save_vocab_fname = os.path.join(args.save_dir, args.save_vocab_fname)\n",
    "\n",
    "    args.data_sizes = \\\n",
    "        select_data(save_dir=args.save_dir, data_dir=args.data_dir,\n",
    "                    train_fname=args.train_fname, data_sizes=args.data_sizes,\n",
    "                    skip_header=True, verbose=True)\n",
    "\n",
    "    if not args.verbose: import pdb; pdb.set_trace = lambda: None\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] #samples in train: 7200, valid: 800, test: 2717\n"
     ]
    }
   ],
   "source": [
    "args = setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'f': '/home/niuhao/.local/share/jupyter/runtime/kernel-fcf17709-a65c-4a9c-ac13-4ff09331c3c5.json',\n",
       " 'init_xavier': False,\n",
       " 'emb_dropout': 0.3,\n",
       " 'emb_dim': 100,\n",
       " 'vocab_max_size': 100000,\n",
       " 'lstm_n_layer': 1,\n",
       " 'lstm_dropout': 0.3,\n",
       " 'lstm_dim': 100,\n",
       " 'lstm_combine': 'add',\n",
       " 'n_linear': 1,\n",
       " 'linear_dropout': 0.5,\n",
       " 'n_classes': 2,\n",
       " 'seed': 0,\n",
       " 'batch_size': 10,\n",
       " 'epochs': 100,\n",
       " 'lr': 1.0,\n",
       " 'weight_decay': 1e-05,\n",
       " 'data_dir': 'data/re_semeval/',\n",
       " 'train_fname': 'train.csv',\n",
       " 'data_sizes': [7200, 800, 2717],\n",
       " 'preprocessed': True,\n",
       " 'lower': False,\n",
       " 'uid': '10102031',\n",
       " 'save_dir': 'tmp/',\n",
       " 'save_dir_cp': 'tmp_cp/',\n",
       " 'save_meta_fname': 'tmp/run_meta.txt',\n",
       " 'save_log_fname': 'tmp/run_log.txt',\n",
       " 'save_valid_fname': 'tmp/valid_e00.txt',\n",
       " 'save_vis_fname': 'tmp/example.txt',\n",
       " 'save_model_fname': 'tmp/model',\n",
       " 'save_vocab_fname': 'tmp/vocab.json',\n",
       " 'n_gpus': 1,\n",
       " 'load_model': '',\n",
       " 'verbose': False}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "select_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir =args.data_dir\n",
    "save_dir=args.save_dir\n",
    "train_fname=args.train_fname\n",
    "data_sizes=args.data_sizes\n",
    "skip_header=True\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = ['train', 'valid', 'test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = files[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "read_from = os.path.join(data_dir,\n",
    "                                 train_fname.replace('train', file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'data/re_semeval/train.csv'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "read_from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = '.' + train_fname.split('.')[-1]\n",
    "n_lines = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.csv'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp/train.csv'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "save_to = os.path.join(save_dir, file + suffix)\n",
    "save_to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(read_from) as f:\n",
    "    data = [line for line in f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tgt,input,show_inp,ent1,ent2,id\\n'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\"Product-Producer(e2,e1)\",The ENT_1_START builder ENT_1_END has now completed the ENT_2_START townhouse ENT_2_END .,The ENT_1_START builder ENT_1_END has now completed the ENT_2_START townhouse ENT_2_END .,builder,townhouse,2569'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "header, body = data[:1], data[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(body)\n",
    "data = header + body[:data_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(body[:None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    " fwrite(''.join(data), save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(save_to) as f:\n",
    "    data = [line.strip() for line in f if line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7201"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_lines = len(data) if not skip_header else len(data) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_lines[file] = num_lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] #samples in train: 7200\n"
     ]
    }
   ],
   "source": [
    "writeout = ['{}: {}'.format(*item) for item in n_lines.items()]\n",
    "writeout = ', '.join(writeout)\n",
    "print('[Info] #samples in', writeout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 数据读取与处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torchtext\n",
    "\n",
    "from torchtext.data import Field, RawField, TabularDataset, \\\n",
    "    BucketIterator, Iterator\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from utils import show_time, fwrite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self, proc_id=0, data_dir='tmp/', train_fname='train.csv',\n",
    "                 preprocessed=True, lower=True,\n",
    "                 vocab_max_size=100000, emb_dim=100,\n",
    "                 save_vocab_fname='vocab.json', verbose=True, ):\n",
    "        self.verbose = verbose and (proc_id == 0)\n",
    "        tokenize = lambda x: x.split() if preprocessed else 'spacy'\n",
    "\n",
    "        INPUT = Field(sequential=True, batch_first=True, tokenize=tokenize,\n",
    "                      lower=lower,\n",
    "                      # include_lengths=True,\n",
    "                      )\n",
    "        # TGT = Field(sequential=False, dtype=torch.long, batch_first=True,\n",
    "        #             use_vocab=False)\n",
    "        TGT = Field(sequential=True, batch_first=True)\n",
    "        SHOW_INP = RawField()\n",
    "        fields = [\n",
    "            ('tgt', TGT),\n",
    "            ('input', INPUT),\n",
    "            ('show_inp', SHOW_INP), ]\n",
    "\n",
    "        if self.verbose:\n",
    "            show_time(\"[Info] Start building TabularDataset from: {}{}\"\n",
    "                      .format(data_dir, 'train.csv'))\n",
    "        datasets = TabularDataset.splits(\n",
    "            fields=fields,\n",
    "            path=data_dir,\n",
    "            format=train_fname.rsplit('.')[-1],\n",
    "            train=train_fname,\n",
    "            validation=train_fname.replace('train', 'valid'),\n",
    "            test=train_fname.replace('train', 'test'),\n",
    "            skip_header=True,\n",
    "        )\n",
    "        INPUT.build_vocab(*datasets, max_size=vocab_max_size,\n",
    "                          vectors=GloVe(name='6B', dim=emb_dim),\n",
    "                          unk_init=torch.Tensor.normal_, )\n",
    "        # load_vocab(hard_dosk) like opennmt\n",
    "        # emb_dim = {50, 100}\n",
    "        # Elmo\n",
    "        TGT.build_vocab(*datasets)\n",
    "\n",
    "        self.INPUT = INPUT\n",
    "        self.TGT = TGT\n",
    "        self.train_ds, self.valid_ds, self.test_ds = datasets\n",
    "\n",
    "        if save_vocab_fname and self.verbose:\n",
    "            writeout = {\n",
    "                'tgt_vocab': {\n",
    "                    'itos': TGT.vocab.itos, 'stoi': TGT.vocab.stoi,\n",
    "                },\n",
    "                'input_vocab': {\n",
    "                    'itos': INPUT.vocab.itos, 'stoi': INPUT.vocab.stoi,\n",
    "                },\n",
    "            }\n",
    "            fwrite(json.dumps(writeout, indent=4), save_vocab_fname)\n",
    "\n",
    "        if self.verbose:\n",
    "            msg = \"[Info] Finished building vocab: {} INPUT, {} TGT\" \\\n",
    "                .format(len(INPUT.vocab), len(TGT.vocab))\n",
    "            show_time(msg)\n",
    "\n",
    "    def get_dataloader(self, proc_id=0, n_gpus=1, device=torch.device('cpu'),\n",
    "                       batch_size=64):\n",
    "        def _distribute_dataset(dataset):\n",
    "            n = len(dataset)\n",
    "            part = dataset[n * proc_id // n_gpus: n * (proc_id + 1) // n_gpus]\n",
    "            return torchtext.data.Dataset(part, dataset.fields)\n",
    "\n",
    "        train_ds = _distribute_dataset(self.train_ds)\n",
    "        self.verbose = self.verbose and (proc_id == 0)\n",
    "        train_iter, valid_iter = BucketIterator.splits(\n",
    "            (train_ds, self.valid_ds),\n",
    "            batch_sizes=(batch_size, batch_size),\n",
    "            sort_within_batch=True,\n",
    "            sort_key=lambda x: len(x.input),\n",
    "            device=device\n",
    "        )\n",
    "\n",
    "        test_iter = BucketIterator(\n",
    "            self.test_ds,\n",
    "            batch_size=1,\n",
    "            sort=False,\n",
    "            sort_within_batch=False,\n",
    "            device=device\n",
    "        )\n",
    "        return train_iter, valid_iter, test_iter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_id = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏰ Time: 10112105-54\t[Info] Start building TabularDataset from: tmp/train.csv\n",
      "⏰ Time: 10112105-56\t[Info] Finished building vocab: 24887 INPUT, 21 TGT\n"
     ]
    }
   ],
   "source": [
    "dataset = Dataset(proc_id=proc_id, data_dir=args.save_dir,\n",
    "                      train_fname=args.train_fname,\n",
    "                      preprocessed=args.preprocessed, lower=args.lower,\n",
    "                      vocab_max_size=args.vocab_max_size, emb_dim=args.emb_dim,\n",
    "                      save_vocab_fname=args.save_vocab_fname, verbose=True, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gpus = 1\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_dl, valid_dl, test_dl = \\\n",
    "        dataset.get_dataloader(proc_id=proc_id, n_gpus=n_gpus, device=device,\n",
    "                               batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "720"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dl)   # batch_size == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 10]\n",
       "\t[.tgt]:[torch.LongTensor of size 10x1]\n",
       "\t[.input]:[torch.LongTensor of size 10x25]\n",
       "\t[.show_inp]:['The second ENT_1_START sentence ENT_1_END tells us about heart ENT_2_START disease ENT_2_END , which is an illness of late middle age and old age .', \"The Royal Navy 's newest 1bn GBP ENT_1_START warship ENT_1_END has been handed over to the new ENT_2_START owner ENT_2_END in a formal ceremony .\", 'The ENT_1_START activities ENT_1_END were documented on the ENT_2_START newsreels ENT_2_END of the day , and form part of a new BBC television series .', \"The biggest broadcast was on election eve , when a ENT_1_START hookup ENT_1_END of twenty - six ENT_2_START stations ENT_2_END carried Coolidge 's speech .\", 'Continuous ENT_1_START improvement ENT_1_END starts with ENT_2_START measuring ENT_2_END process performance , and instigating a robust process for reviewing further changes logically and quickly .', 'From time to time a ENT_1_START drove ENT_1_END of ENT_2_START goats ENT_2_END or donkeys passed by on its way to the Jardin du Luxembourg .', 'It is a 1956 crime ENT_1_START drama ENT_1_END examining the ENT_2_START reactions ENT_2_END of parents , police , and the public to a kidnapping .', 'A brewer from Southwick is immortalised as a Hampshire ENT_1_START farmer ENT_1_END produces a new real ENT_2_START ale ENT_2_END in celebration of Old Dick .', 'A ENT_1_START writer ENT_1_END who lives in Salt Lake City has recently completed a ENT_2_START memoir ENT_2_END about her experiences as a Mormon missionary .', 'Peter Wayner , a ENT_1_START technology ENT_1_END ENT_2_START writer ENT_2_END , struggles with how to respond to the widespread piracy of his books online .']"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in train_dl][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_id=proc_id    #0\n",
    "data_dir=args.save_dir\n",
    "train_fname=args.train_fname\n",
    "preprocessed=args.preprocessed    # True\n",
    "lower=args.lower   # False\n",
    "vocab_max_size=args.vocab_max_size    # 100000\n",
    "emb_dim=args.emb_dim    #(100,)\n",
    "save_vocab_fname=args.save_vocab_fname   #'tmp/vocab.json'\n",
    "verbose=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = verbose and (proc_id == 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenize = lambda x: x.split() if preprocessed else 'spacy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = Field(sequential=True, batch_first=True, tokenize=tokenize,\n",
    "                      lower=lower)       #数据预处理配置信息"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "TGT = Field(sequential=True, batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "SHOW_INP = RawField()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "fields = [\n",
    "            ('tgt', TGT),\n",
    "            ('input', INPUT),\n",
    "            ('show_inp', SHOW_INP), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏰ Time: 10112014-31\t[Info] Start building TabularDataset from: tmp/train.csv\n"
     ]
    }
   ],
   "source": [
    "if verbose:\n",
    "    show_time(\"[Info] Start building TabularDataset from: {}{}\"\n",
    "              .format(data_dir, 'train.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp/'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train.csv'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'valid.csv'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_fname.replace('train', 'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = TabularDataset.splits(\n",
    "            fields=fields,\n",
    "            path=data_dir,\n",
    "            format=train_fname.rsplit('.')[-1],\n",
    "            train=train_fname,\n",
    "            validation=train_fname.replace('train', 'valid'),\n",
    "            test=train_fname.replace('train', 'test'),\n",
    "            skip_header=True,  \n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<torchtext.data.dataset.TabularDataset at 0x7fee828cac70>,\n",
       " <torchtext.data.dataset.TabularDataset at 0x7fee828cae50>,\n",
       " <torchtext.data.dataset.TabularDataset at 0x7fef0d49f0d0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['examples', 'fields'])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].__dict__.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tgt': <torchtext.data.field.Field at 0x7febe3f03c70>,\n",
       " 'input': <torchtext.data.field.Field at 0x7febe3f03ca0>,\n",
       " 'show_inp': <torchtext.data.field.RawField at 0x7febe3f03430>}"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].fields"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ENT_1_START',\n",
       " 'Paralysis',\n",
       " 'ENT_1_END',\n",
       " 'or',\n",
       " 'convulsions',\n",
       " 'are',\n",
       " 'caused',\n",
       " 'by',\n",
       " 'hormone',\n",
       " 'deficiencies',\n",
       " 'and',\n",
       " 'ENT_2_START',\n",
       " 'imbalances',\n",
       " 'ENT_2_END',\n",
       " '.']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datasets[0].examples[0].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT.build_vocab(*datasets, max_size=vocab_max_size,\n",
    "                          vectors=GloVe(name='6B', dim=emb_dim),\n",
    "                          unk_init=torch.Tensor.normal_, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fee7b3774f0>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             'the': 2,\n",
       "             'ENT_1_END': 3,\n",
       "             'ENT_1_START': 4,\n",
       "             'ENT_2_END': 5,\n",
       "             'ENT_2_START': 6,\n",
       "             '.': 7,\n",
       "             'of': 8,\n",
       "             'a': 9,\n",
       "             ',': 10,\n",
       "             'and': 11,\n",
       "             'The': 12,\n",
       "             'in': 13,\n",
       "             'to': 14,\n",
       "             'is': 15,\n",
       "             'was': 16,\n",
       "             'from': 17,\n",
       "             'by': 18,\n",
       "             'with': 19,\n",
       "             '-': 20,\n",
       "             'on': 21,\n",
       "             'that': 22,\n",
       "             'into': 23,\n",
       "             'for': 24,\n",
       "             \"'s\": 25,\n",
       "             'an': 26,\n",
       "             'are': 27,\n",
       "             'as': 28,\n",
       "             'has': 29,\n",
       "             'have': 30,\n",
       "             'A': 31,\n",
       "             'it': 32,\n",
       "             'at': 33,\n",
       "             'his': 34,\n",
       "             'caused': 35,\n",
       "             '\"': 36,\n",
       "             'or': 37,\n",
       "             'I': 38,\n",
       "             'which': 39,\n",
       "             'were': 40,\n",
       "             'been': 41,\n",
       "             'this': 42,\n",
       "             'This': 43,\n",
       "             'their': 44,\n",
       "             ')': 45,\n",
       "             '(': 46,\n",
       "             'had': 47,\n",
       "             'In': 48,\n",
       "             'one': 49,\n",
       "             'who': 50,\n",
       "             'out': 51,\n",
       "             'made': 52,\n",
       "             'up': 53,\n",
       "             'its': 54,\n",
       "             'inside': 55,\n",
       "             'he': 56,\n",
       "             'other': 57,\n",
       "             'about': 58,\n",
       "             'her': 59,\n",
       "             'be': 60,\n",
       "             'after': 61,\n",
       "             'all': 62,\n",
       "             'but': 63,\n",
       "             'first': 64,\n",
       "             'my': 65,\n",
       "             'two': 66,\n",
       "             'they': 67,\n",
       "             'over': 68,\n",
       "             \"'\": 69,\n",
       "             'It': 70,\n",
       "             'also': 71,\n",
       "             'He': 72,\n",
       "             'not': 73,\n",
       "             'new': 74,\n",
       "             'through': 75,\n",
       "             'some': 76,\n",
       "             'when': 77,\n",
       "             'more': 78,\n",
       "             'water': 79,\n",
       "             'people': 80,\n",
       "             'used': 81,\n",
       "             'time': 82,\n",
       "             'book': 83,\n",
       "             'years': 84,\n",
       "             'put': 85,\n",
       "             'We': 86,\n",
       "             'like': 87,\n",
       "             'than': 88,\n",
       "             'so': 89,\n",
       "             'them': 90,\n",
       "             'using': 91,\n",
       "             'we': 92,\n",
       "             'most': 93,\n",
       "             'only': 94,\n",
       "             'small': 95,\n",
       "             'many': 96,\n",
       "             'company': 97,\n",
       "             'part': 98,\n",
       "             'found': 99,\n",
       "             'you': 100,\n",
       "             'can': 101,\n",
       "             'such': 102,\n",
       "             'cause': 103,\n",
       "             'left': 104,\n",
       "             'our': 105,\n",
       "             'large': 106,\n",
       "             'placed': 107,\n",
       "             'three': 108,\n",
       "             'between': 109,\n",
       "             'box': 110,\n",
       "             'use': 111,\n",
       "             'system': 112,\n",
       "             'well': 113,\n",
       "             'year': 114,\n",
       "             'then': 115,\n",
       "             'man': 116,\n",
       "             'body': 117,\n",
       "             'where': 118,\n",
       "             'these': 119,\n",
       "             'work': 120,\n",
       "             'produced': 121,\n",
       "             'your': 122,\n",
       "             'back': 123,\n",
       "             'way': 124,\n",
       "             'being': 125,\n",
       "             'around': 126,\n",
       "             'away': 127,\n",
       "             'just': 128,\n",
       "             'off': 129,\n",
       "             'world': 130,\n",
       "             'old': 131,\n",
       "             'there': 132,\n",
       "             'very': 133,\n",
       "             'high': 134,\n",
       "             'before': 135,\n",
       "             'each': 136,\n",
       "             'As': 137,\n",
       "             'contained': 138,\n",
       "             'bottle': 139,\n",
       "             'different': 140,\n",
       "             'full': 141,\n",
       "             'him': 142,\n",
       "             'They': 143,\n",
       "             'under': 144,\n",
       "             ';': 145,\n",
       "             'life': 146,\n",
       "             'during': 147,\n",
       "             'down': 148,\n",
       "             'long': 149,\n",
       "             'took': 150,\n",
       "             'set': 151,\n",
       "             'last': 152,\n",
       "             'These': 153,\n",
       "             'several': 154,\n",
       "             'various': 155,\n",
       "             'There': 156,\n",
       "             'oil': 157,\n",
       "             'area': 158,\n",
       "             'When': 159,\n",
       "             'car': 160,\n",
       "             'great': 161,\n",
       "             'day': 162,\n",
       "             'hand': 163,\n",
       "             'while': 164,\n",
       "             'local': 165,\n",
       "             'building': 166,\n",
       "             'now': 167,\n",
       "             'number': 168,\n",
       "             'right': 169,\n",
       "             'An': 170,\n",
       "             'same': 171,\n",
       "             'side': 172,\n",
       "             'My': 173,\n",
       "             'few': 174,\n",
       "             'paper': 175,\n",
       "             'city': 176,\n",
       "             'information': 177,\n",
       "             'those': 178,\n",
       "             'government': 179,\n",
       "             'make': 180,\n",
       "             'best': 181,\n",
       "             'both': 182,\n",
       "             'light': 183,\n",
       "             'order': 184,\n",
       "             'team': 185,\n",
       "             'His': 186,\n",
       "             'little': 187,\n",
       "             'room': 188,\n",
       "             'country': 189,\n",
       "             'group': 190,\n",
       "             'how': 191,\n",
       "             'power': 192,\n",
       "             'public': 193,\n",
       "             'shows': 194,\n",
       "             'end': 195,\n",
       "             'bag': 196,\n",
       "             'because': 197,\n",
       "             'called': 198,\n",
       "             'came': 199,\n",
       "             'study': 200,\n",
       "             'against': 201,\n",
       "             'causes': 202,\n",
       "             'contains': 203,\n",
       "             'head': 204,\n",
       "             'help': 205,\n",
       "             'house': 206,\n",
       "             'money': 207,\n",
       "             'no': 208,\n",
       "             'sent': 209,\n",
       "             'she': 210,\n",
       "             'what': 211,\n",
       "             ':': 212,\n",
       "             'family': 213,\n",
       "             'home': 214,\n",
       "             'human': 215,\n",
       "             'makes': 216,\n",
       "             'own': 217,\n",
       "             'case': 218,\n",
       "             'created': 219,\n",
       "             'got': 220,\n",
       "             'suitcase': 221,\n",
       "             'air': 222,\n",
       "             'comes': 223,\n",
       "             'making': 224,\n",
       "             'poured': 225,\n",
       "             'another': 226,\n",
       "             'do': 227,\n",
       "             'following': 228,\n",
       "             'get': 229,\n",
       "             'young': 230,\n",
       "             'ago': 231,\n",
       "             'early': 232,\n",
       "             'even': 233,\n",
       "             'four': 234,\n",
       "             'much': 235,\n",
       "             'products': 236,\n",
       "             'students': 237,\n",
       "             'top': 238,\n",
       "             'food': 239,\n",
       "             'machine': 240,\n",
       "             'place': 241,\n",
       "             'report': 242,\n",
       "             'wine': 243,\n",
       "             'any': 244,\n",
       "             'damage': 245,\n",
       "             'moved': 246,\n",
       "             'including': 247,\n",
       "             'story': 248,\n",
       "             'One': 249,\n",
       "             'author': 250,\n",
       "             'built': 251,\n",
       "             'good': 252,\n",
       "             'plastic': 253,\n",
       "             'second': 254,\n",
       "             'still': 255,\n",
       "             'New': 256,\n",
       "             'front': 257,\n",
       "             'known': 258,\n",
       "             'next': 259,\n",
       "             'source': 260,\n",
       "             'uses': 261,\n",
       "             \"n't\": 262,\n",
       "             'state': 263,\n",
       "             'released': 264,\n",
       "             'series': 265,\n",
       "             'tree': 266,\n",
       "             'did': 267,\n",
       "             'me': 268,\n",
       "             'research': 269,\n",
       "             'started': 270,\n",
       "             '/': 271,\n",
       "             'After': 272,\n",
       "             'along': 273,\n",
       "             'blood': 274,\n",
       "             'children': 275,\n",
       "             'could': 276,\n",
       "             'death': 277,\n",
       "             'disease': 278,\n",
       "             'fire': 279,\n",
       "             'person': 280,\n",
       "             'skin': 281,\n",
       "             'She': 282,\n",
       "             'century': 283,\n",
       "             'computer': 284,\n",
       "             'near': 285,\n",
       "             'process': 286,\n",
       "             'school': 287,\n",
       "             'student': 288,\n",
       "             'white': 289,\n",
       "             'On': 290,\n",
       "             'arrived': 291,\n",
       "             'article': 292,\n",
       "             'common': 293,\n",
       "             'countries': 294,\n",
       "             'glass': 295,\n",
       "             'main': 296,\n",
       "             'based': 297,\n",
       "             'derived': 298,\n",
       "             'issues': 299,\n",
       "             'method': 300,\n",
       "             'past': 301,\n",
       "             'police': 302,\n",
       "             'went': 303,\n",
       "             'within': 304,\n",
       "             'American': 305,\n",
       "             'For': 306,\n",
       "             'books': 307,\n",
       "             'form': 308,\n",
       "             'major': 309,\n",
       "             'open': 310,\n",
       "             'since': 311,\n",
       "             'train': 312,\n",
       "             'data': 313,\n",
       "             'developed': 314,\n",
       "             'resulted': 315,\n",
       "             'see': 316,\n",
       "             'ship': 317,\n",
       "             'space': 318,\n",
       "             'without': 319,\n",
       "             'come': 320,\n",
       "             'days': 321,\n",
       "             'door': 322,\n",
       "             'factory': 323,\n",
       "             'film': 324,\n",
       "             'gas': 325,\n",
       "             'often': 326,\n",
       "             'plant': 327,\n",
       "             'running': 328,\n",
       "             'saw': 329,\n",
       "             'women': 330,\n",
       "             '$': 331,\n",
       "             '%': 332,\n",
       "             'comprises': 333,\n",
       "             'later': 334,\n",
       "             'music': 335,\n",
       "             'political': 336,\n",
       "             'short': 337,\n",
       "             'speech': 338,\n",
       "             'subject': 339,\n",
       "             'take': 340,\n",
       "             'arm': 341,\n",
       "             'cells': 342,\n",
       "             'energy': 343,\n",
       "             'going': 344,\n",
       "             'important': 345,\n",
       "             'metal': 346,\n",
       "             'original': 347,\n",
       "             'show': 348,\n",
       "             'single': 349,\n",
       "             'wall': 350,\n",
       "             'will': 351,\n",
       "             'woman': 352,\n",
       "             'across': 353,\n",
       "             'generated': 354,\n",
       "             'heart': 355,\n",
       "             'land': 356,\n",
       "             'pain': 357,\n",
       "             'present': 358,\n",
       "             'works': 359,\n",
       "             'big': 360,\n",
       "             'fish': 361,\n",
       "             'history': 362,\n",
       "             'us': 363,\n",
       "             'war': 364,\n",
       "             'written': 365,\n",
       "             'among': 366,\n",
       "             'brought': 367,\n",
       "             'carried': 368,\n",
       "             'every': 369,\n",
       "             'kitchen': 370,\n",
       "             'night': 371,\n",
       "             'stored': 372,\n",
       "             '10': 373,\n",
       "             'But': 374,\n",
       "             'black': 375,\n",
       "             'field': 376,\n",
       "             'floor': 377,\n",
       "             'maker': 378,\n",
       "             'party': 379,\n",
       "             'pressure': 380,\n",
       "             'until': 381,\n",
       "             'working': 382,\n",
       "             'would': 383,\n",
       "             '1': 384,\n",
       "             'earthquake': 385,\n",
       "             'find': 386,\n",
       "             'five': 387,\n",
       "             'game': 388,\n",
       "             'if': 389,\n",
       "             'kept': 390,\n",
       "             'law': 391,\n",
       "             'name': 392,\n",
       "             'point': 393,\n",
       "             'recent': 394,\n",
       "             'together': 395,\n",
       "             'At': 396,\n",
       "             'behind': 397,\n",
       "             'composed': 398,\n",
       "             'control': 399,\n",
       "             'morning': 400,\n",
       "             'plane': 401,\n",
       "             'problems': 402,\n",
       "             'program': 403,\n",
       "             'ran': 404,\n",
       "             'triggered': 405,\n",
       "             'Many': 406,\n",
       "             'Some': 407,\n",
       "             'added': 408,\n",
       "             'bed': 409,\n",
       "             'business': 410,\n",
       "             'given': 411,\n",
       "             'health': 412,\n",
       "             'letter': 413,\n",
       "             'office': 414,\n",
       "             'taken': 415,\n",
       "             'week': 416,\n",
       "             'became': 417,\n",
       "             'drug': 418,\n",
       "             'filled': 419,\n",
       "             'locked': 420,\n",
       "             'low': 421,\n",
       "             'material': 422,\n",
       "             'production': 423,\n",
       "             'red': 424,\n",
       "             'site': 425,\n",
       "             'social': 426,\n",
       "             'store': 427,\n",
       "             'stress': 428,\n",
       "             'term': 429,\n",
       "             'workers': 430,\n",
       "             'To': 431,\n",
       "             'bacteria': 432,\n",
       "             'boy': 433,\n",
       "             'brain': 434,\n",
       "             'child': 435,\n",
       "             'community': 436,\n",
       "             'current': 437,\n",
       "             'due': 438,\n",
       "             'experience': 439,\n",
       "             'having': 440,\n",
       "             'held': 441,\n",
       "             'moving': 442,\n",
       "             'natural': 443,\n",
       "             'parts': 444,\n",
       "             'published': 445,\n",
       "             'result': 446,\n",
       "             'results': 447,\n",
       "             'sound': 448,\n",
       "             'whole': 449,\n",
       "             'above': 450,\n",
       "             'boat': 451,\n",
       "             'development': 452,\n",
       "             'device': 453,\n",
       "             'discussion': 454,\n",
       "             'lead': 455,\n",
       "             'president': 456,\n",
       "             'real': 457,\n",
       "             'software': 458,\n",
       "             'statement': 459,\n",
       "             'structure': 460,\n",
       "             'Our': 461,\n",
       "             'University': 462,\n",
       "             'already': 463,\n",
       "             'completed': 464,\n",
       "             'delivered': 465,\n",
       "             'discovered': 466,\n",
       "             'enclosed': 467,\n",
       "             'ground': 468,\n",
       "             'includes': 469,\n",
       "             'itself': 470,\n",
       "             'less': 471,\n",
       "             'meeting': 472,\n",
       "             'patient': 473,\n",
       "             'related': 474,\n",
       "             'species': 475,\n",
       "             'starts': 476,\n",
       "             '5': 477,\n",
       "             'areas': 478,\n",
       "             'artist': 479,\n",
       "             'bottom': 480,\n",
       "             'cell': 481,\n",
       "             'changes': 482,\n",
       "             'cup': 483,\n",
       "             'exhibition': 484,\n",
       "             'fuel': 485,\n",
       "             'growth': 486,\n",
       "             'here': 487,\n",
       "             'hidden': 488,\n",
       "             'hole': 489,\n",
       "             'industry': 490,\n",
       "             'literature': 491,\n",
       "             'living': 492,\n",
       "             'lower': 493,\n",
       "             'member': 494,\n",
       "             'outside': 495,\n",
       "             'passed': 496,\n",
       "             'play': 497,\n",
       "             'product': 498,\n",
       "             'rather': 499,\n",
       "             'region': 500,\n",
       "             'safe': 501,\n",
       "             'similar': 502,\n",
       "             'times': 503,\n",
       "             'too': 504,\n",
       "             'usually': 505,\n",
       "             'British': 506,\n",
       "             'Most': 507,\n",
       "             'acid': 508,\n",
       "             'almost': 509,\n",
       "             'become': 510,\n",
       "             'care': 511,\n",
       "             'future': 512,\n",
       "             'hours': 513,\n",
       "             'list': 514,\n",
       "             'once': 515,\n",
       "             'picture': 516,\n",
       "             'presented': 517,\n",
       "             'virus': 518,\n",
       "             '--': 519,\n",
       "             '30': 520,\n",
       "             'amount': 521,\n",
       "             'course': 522,\n",
       "             'design': 523,\n",
       "             'designed': 524,\n",
       "             'feet': 525,\n",
       "             'gave': 526,\n",
       "             'hands': 527,\n",
       "             'international': 528,\n",
       "             'key': 529,\n",
       "             'period': 530,\n",
       "             'piece': 531,\n",
       "             'run': 532,\n",
       "             'science': 533,\n",
       "             'third': 534,\n",
       "             'thrown': 535,\n",
       "             'topic': 536,\n",
       "             'unit': 537,\n",
       "             'army': 538,\n",
       "             'birds': 539,\n",
       "             'described': 540,\n",
       "             'does': 541,\n",
       "             'engine': 542,\n",
       "             'fell': 543,\n",
       "             'free': 544,\n",
       "             'inserted': 545,\n",
       "             'men': 546,\n",
       "             'months': 547,\n",
       "             'need': 548,\n",
       "             'player': 549,\n",
       "             'purpose': 550,\n",
       "             'range': 551,\n",
       "             'section': 552,\n",
       "             'seen': 553,\n",
       "             'service': 554,\n",
       "             'signal': 555,\n",
       "             'temperature': 556,\n",
       "             'today': 557,\n",
       "             'turned': 558,\n",
       "             'weeks': 559,\n",
       "             'China': 560,\n",
       "             'United': 561,\n",
       "             'accident': 562,\n",
       "             'account': 563,\n",
       "             'attached': 564,\n",
       "             'baby': 565,\n",
       "             'basket': 566,\n",
       "             'church': 567,\n",
       "             'constructed': 568,\n",
       "             'effect': 569,\n",
       "             'entire': 570,\n",
       "             'female': 571,\n",
       "             'hold': 572,\n",
       "             'included': 573,\n",
       "             'killed': 574,\n",
       "             'largest': 575,\n",
       "             'line': 576,\n",
       "             'market': 577,\n",
       "             'may': 578,\n",
       "             'medical': 579,\n",
       "             'million': 580,\n",
       "             'news': 581,\n",
       "             'removed': 582,\n",
       "             'scientists': 583,\n",
       "             'six': 584,\n",
       "             'size': 585,\n",
       "             'surface': 586,\n",
       "             'takes': 587,\n",
       "             'tool': 588,\n",
       "             'And': 589,\n",
       "             'North': 590,\n",
       "             'South': 591,\n",
       "             'Two': 592,\n",
       "             'With': 593,\n",
       "             'beer': 594,\n",
       "             'bought': 595,\n",
       "             'committee': 596,\n",
       "             'document': 597,\n",
       "             'economic': 598,\n",
       "             'established': 599,\n",
       "             'events': 600,\n",
       "             'flight': 601,\n",
       "             'friend': 602,\n",
       "             'gives': 603,\n",
       "             'job': 604,\n",
       "             'live': 605,\n",
       "             'mother': 606,\n",
       "             'outer': 607,\n",
       "             'previous': 608,\n",
       "             'release': 609,\n",
       "             'ring': 610,\n",
       "             'sea': 611,\n",
       "             'sealed': 612,\n",
       "             'staff': 613,\n",
       "             'table': 614,\n",
       "             'television': 615,\n",
       "             'town': 616,\n",
       "             'upon': 617,\n",
       "             'user': 618,\n",
       "             '2': 619,\n",
       "             'US': 620,\n",
       "             'began': 621,\n",
       "             'couple': 622,\n",
       "             'driver': 623,\n",
       "             'dropped': 624,\n",
       "             'face': 625,\n",
       "             'famous': 626,\n",
       "             'flour': 627,\n",
       "             'former': 628,\n",
       "             'general': 629,\n",
       "             'infection': 630,\n",
       "             'minutes': 631,\n",
       "             'novel': 632,\n",
       "             'popular': 633,\n",
       "             'population': 634,\n",
       "             'problem': 635,\n",
       "             'really': 636,\n",
       "             'style': 637,\n",
       "             'sugar': 638,\n",
       "             'support': 639,\n",
       "             'tank': 640,\n",
       "             'text': 641,\n",
       "             'theory': 642,\n",
       "             'truck': 643,\n",
       "             'vehicle': 644,\n",
       "             'vessel': 645,\n",
       "             'writer': 646,\n",
       "             \"'m\": 647,\n",
       "             'All': 648,\n",
       "             'That': 649,\n",
       "             'am': 650,\n",
       "             'animal': 651,\n",
       "             'beautiful': 652,\n",
       "             'cut': 653,\n",
       "             'enough': 654,\n",
       "             'entered': 655,\n",
       "             'farm': 656,\n",
       "             'girl': 657,\n",
       "             'idea': 658,\n",
       "             'jar': 659,\n",
       "             'leading': 660,\n",
       "             'loss': 661,\n",
       "             'message': 662,\n",
       "             'prepared': 663,\n",
       "             'project': 664,\n",
       "             'provide': 665,\n",
       "             'provided': 666,\n",
       "             'researchers': 667,\n",
       "             'scene': 668,\n",
       "             'soil': 669,\n",
       "             'station': 670,\n",
       "             'wooden': 671,\n",
       "             '3': 672,\n",
       "             'However': 673,\n",
       "             'World': 674,\n",
       "             'addition': 675,\n",
       "             'again': 676,\n",
       "             'available': 677,\n",
       "             'below': 678,\n",
       "             'blue': 679,\n",
       "             'change': 680,\n",
       "             'contents': 681,\n",
       "             'cover': 682,\n",
       "             'crisis': 683,\n",
       "             'deep': 684,\n",
       "             'distilled': 685,\n",
       "             'features': 686,\n",
       "             'final': 687,\n",
       "             'half': 688,\n",
       "             'least': 689,\n",
       "             'press': 690,\n",
       "             'questions': 691,\n",
       "             'radiation': 692,\n",
       "             'received': 693,\n",
       "             'road': 694,\n",
       "             'society': 695,\n",
       "             'strong': 696,\n",
       "             'wide': 697,\n",
       "             \"'ve\": 698,\n",
       "             'Each': 699,\n",
       "             'European': 700,\n",
       "             'York': 701,\n",
       "             'access': 702,\n",
       "             'applied': 703,\n",
       "             'band': 704,\n",
       "             'better': 705,\n",
       "             'center': 706,\n",
       "             'coming': 707,\n",
       "             'companies': 708,\n",
       "             'driven': 709,\n",
       "             'example': 710,\n",
       "             'eye': 711,\n",
       "             'force': 712,\n",
       "             'handle': 713,\n",
       "             'include': 714,\n",
       "             'level': 715,\n",
       "             'looking': 716,\n",
       "             'love': 717,\n",
       "             'movie': 718,\n",
       "             'nature': 719,\n",
       "             'needs': 720,\n",
       "             'private': 721,\n",
       "             'river': 722,\n",
       "             'simple': 723,\n",
       "             'song': 724,\n",
       "             'special': 725,\n",
       "             'standard': 726,\n",
       "             'things': 727,\n",
       "             'tube': 728,\n",
       "             'variety': 729,\n",
       "             'website': 730,\n",
       "             'wind': 731,\n",
       "             'Another': 732,\n",
       "             'French': 733,\n",
       "             'John': 734,\n",
       "             'States': 735,\n",
       "             'analysis': 736,\n",
       "             'ancient': 737,\n",
       "             'ball': 738,\n",
       "             'bomb': 739,\n",
       "             'build': 740,\n",
       "             'coffee': 741,\n",
       "             'consists': 742,\n",
       "             'construction': 743,\n",
       "             'court': 744,\n",
       "             'create': 745,\n",
       "             'debate': 746,\n",
       "             'double': 747,\n",
       "             'ear': 748,\n",
       "             'environmental': 749,\n",
       "             'far': 750,\n",
       "             'fruit': 751,\n",
       "             'gold': 752,\n",
       "             'however': 753,\n",
       "             'items': 754,\n",
       "             'language': 755,\n",
       "             'look': 756,\n",
       "             'online': 757,\n",
       "             'personal': 758,\n",
       "             'plan': 759,\n",
       "             'produce': 760,\n",
       "             'quite': 761,\n",
       "             'recently': 762,\n",
       "             'teacher': 763,\n",
       "             'technology': 764,\n",
       "             'type': 765,\n",
       "             'view': 766,\n",
       "             'September': 767,\n",
       "             'Since': 768,\n",
       "             'able': 769,\n",
       "             'activities': 770,\n",
       "             'animals': 771,\n",
       "             'arrival': 772,\n",
       "             'art': 773,\n",
       "             'cancer': 774,\n",
       "             'class': 775,\n",
       "             'cold': 776,\n",
       "             'dust': 777,\n",
       "             'event': 778,\n",
       "             'father': 779,\n",
       "             'fear': 780,\n",
       "             'friends': 781,\n",
       "             'further': 782,\n",
       "             'global': 783,\n",
       "             'green': 784,\n",
       "             'growing': 785,\n",
       "             'leaked': 786,\n",
       "             'materials': 787,\n",
       "             'modern': 788,\n",
       "             'mouth': 789,\n",
       "             'movement': 790,\n",
       "             'must': 791,\n",
       "             'non': 792,\n",
       "             'papers': 793,\n",
       "             'position': 794,\n",
       "             'quality': 795,\n",
       "             'raised': 796,\n",
       "             'rock': 797,\n",
       "             'said': 798,\n",
       "             'studies': 799,\n",
       "             'systems': 800,\n",
       "             'taking': 801,\n",
       "             'test': 802,\n",
       "             'themselves': 803,\n",
       "             'thought': 804,\n",
       "             'trees': 805,\n",
       "             'video': 806,\n",
       "             'window': 807,\n",
       "             'wood': 808,\n",
       "             'words': 809,\n",
       "             'Africa': 810,\n",
       "             'So': 811,\n",
       "             'bank': 812,\n",
       "             'basic': 813,\n",
       "             'bus': 814,\n",
       "             'cabinet': 815,\n",
       "             'cars': 816,\n",
       "             'cases': 817,\n",
       "             'caught': 818,\n",
       "             'character': 819,\n",
       "             'clear': 820,\n",
       "             'connected': 821,\n",
       "             'creates': 822,\n",
       "             'dog': 823,\n",
       "             'earth': 824,\n",
       "             'economy': 825,\n",
       "             'edge': 826,\n",
       "             'ever': 827,\n",
       "             'fever': 828,\n",
       "             'flu': 829,\n",
       "             'increased': 830,\n",
       "             'leaves': 831,\n",
       "             'military': 832,\n",
       "             'model': 833,\n",
       "             'move': 834,\n",
       "             'named': 835,\n",
       "             'national': 836,\n",
       "             'opened': 837,\n",
       "             'others': 838,\n",
       "             'plants': 839,\n",
       "             'post': 840,\n",
       "             'role': 841,\n",
       "             'runs': 842,\n",
       "             'stick': 843,\n",
       "             'stone': 844,\n",
       "             'tea': 845,\n",
       "             'threw': 846,\n",
       "             'toward': 847,\n",
       "             'treatment': 848,\n",
       "             'trunk': 849,\n",
       "             'tsunami': 850,\n",
       "             'village': 851,\n",
       "             'wheel': 852,\n",
       "             'word': 853,\n",
       "             'worker': 854,\n",
       "             'Here': 855,\n",
       "             'U.S.': 856,\n",
       "             'You': 857,\n",
       "             'action': 858,\n",
       "             'age': 859,\n",
       "             'alcohol': 860,\n",
       "             'base': 861,\n",
       "             'blade': 862,\n",
       "             'camera': 863,\n",
       "             'conference': 864,\n",
       "             'covered': 865,\n",
       "             'drugs': 866,\n",
       "             'earlier': 867,\n",
       "             'equipment': 868,\n",
       "             'fall': 869,\n",
       "             'frame': 870,\n",
       "             'fresh': 871,\n",
       "             'give': 872,\n",
       "             'gun': 873,\n",
       "             'handed': 874,\n",
       "             'hard': 875,\n",
       "             'heat': 876,\n",
       "             'huge': 877,\n",
       "             'ice': 878,\n",
       "             'increase': 879,\n",
       "             'injury': 880,\n",
       "             'issue': 881,\n",
       "             'leaving': 882,\n",
       "             'lid': 883,\n",
       "             'lives': 884,\n",
       "             'located': 885,\n",
       "             'mail': 886,\n",
       "             'means': 887,\n",
       "             'memory': 888,\n",
       "             'migrated': 889,\n",
       "             'milk': 890,\n",
       "             'month': 891,\n",
       "             'oven': 892,\n",
       "             'page': 893,\n",
       "             'phone': 894,\n",
       "             'poverty': 895,\n",
       "             'provides': 896,\n",
       "             'radio': 897,\n",
       "             'screen': 898,\n",
       "             'shop': 899,\n",
       "             'sold': 900,\n",
       "             'states': 901,\n",
       "             'stem': 902,\n",
       "             'stomach': 903,\n",
       "             'storm': 904,\n",
       "             'sun': 905,\n",
       "             'thus': 906,\n",
       "             'tools': 907,\n",
       "             'turn': 908,\n",
       "             'whose': 909,\n",
       "             'Chinese': 910,\n",
       "             'During': 911,\n",
       "             'English': 912,\n",
       "             'India': 913,\n",
       "             'Their': 914,\n",
       "             'academic': 915,\n",
       "             'actually': 916,\n",
       "             'aircraft': 917,\n",
       "             'always': 918,\n",
       "             'bar': 919,\n",
       "             'bees': 920,\n",
       "             'button': 921,\n",
       "             'chapter': 922,\n",
       "             'chief': 923,\n",
       "             'climate': 924,\n",
       "             'considered': 925,\n",
       "             'creation': 926,\n",
       "             'dedicated': 927,\n",
       "             'drive': 928,\n",
       "             'education': 929,\n",
       "             'farmed': 930,\n",
       "             'formed': 931,\n",
       "             'forms': 932,\n",
       "             'instrument': 933,\n",
       "             'liquid': 934,\n",
       "             'mouse': 935,\n",
       "             'nose': 936,\n",
       "             'obtained': 937,\n",
       "             'pollution': 938,\n",
       "             'poor': 939,\n",
       "             'produces': 940,\n",
       "             'professional': 941,\n",
       "             'responsible': 942,\n",
       "             'sample': 943,\n",
       "             'security': 944,\n",
       "             'something': 945,\n",
       "             'specific': 946,\n",
       "             'spread': 947,\n",
       "             'steel': 948,\n",
       "             'tissue': 949,\n",
       "             'vaccine': 950,\n",
       "             'wild': 951,\n",
       "             'worked': 952,\n",
       "             'writing': 953,\n",
       "             '4': 954,\n",
       "             '6': 955,\n",
       "             'By': 956,\n",
       "             'Europe': 957,\n",
       "             'From': 958,\n",
       "             'Italian': 959,\n",
       "             'Then': 960,\n",
       "             'authors': 961,\n",
       "             'bit': 962,\n",
       "             'bone': 963,\n",
       "             'bridge': 964,\n",
       "             'call': 965,\n",
       "             'central': 966,\n",
       "             'code': 967,\n",
       "             'concerned': 968,\n",
       "             'container': 969,\n",
       "             'crowd': 970,\n",
       "             'destruction': 971,\n",
       "             'developing': 972,\n",
       "             'done': 973,\n",
       "             'easily': 974,\n",
       "             'electricity': 975,\n",
       "             'go': 976,\n",
       "             'hot': 977,\n",
       "             'impact': 978,\n",
       "             'involved': 979,\n",
       "             'lake': 980,\n",
       "             'late': 981,\n",
       "             'magazine': 982,\n",
       "             'management': 983,\n",
       "             'members': 984,\n",
       "             'motor': 985,\n",
       "             'neck': 986,\n",
       "             'organization': 987,\n",
       "             'painting': 988,\n",
       "             'primary': 989,\n",
       "             'pulled': 990,\n",
       "             'rain': 991,\n",
       "             'read': 992,\n",
       "             'review': 993,\n",
       "             'shown': 994,\n",
       "             'smoke': 995,\n",
       "             'tiny': 996,\n",
       "             'towards': 997,\n",
       "             'traditional': 998,\n",
       "             'tried': 999,\n",
       "             ...})"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24887, 100])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "INPUT.vocab.vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "TGT.build_vocab(*datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT = INPUT\n",
    "TGT = TGT\n",
    "train_ds, valid_ds, test_ds = datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(<bound method Vocab._default_unk_index of <torchtext.vocab.Vocab object at 0x7fef0d498fa0>>,\n",
       "            {'<unk>': 0,\n",
       "             '<pad>': 1,\n",
       "             'Other': 2,\n",
       "             'Entity-Destination(e1,e2)': 3,\n",
       "             'Cause-Effect(e2,e1)': 4,\n",
       "             'Member-Collection(e2,e1)': 5,\n",
       "             'Entity-Origin(e1,e2)': 6,\n",
       "             'Message-Topic(e1,e2)': 7,\n",
       "             'Component-Whole(e1,e2)': 8,\n",
       "             'Component-Whole(e2,e1)': 9,\n",
       "             'Instrument-Agency(e2,e1)': 10,\n",
       "             'Content-Container(e1,e2)': 11,\n",
       "             'Product-Producer(e2,e1)': 12,\n",
       "             'Cause-Effect(e1,e2)': 13,\n",
       "             'Product-Producer(e1,e2)': 14,\n",
       "             'Content-Container(e2,e1)': 15,\n",
       "             'Entity-Origin(e2,e1)': 16,\n",
       "             'Message-Topic(e2,e1)': 17,\n",
       "             'Instrument-Agency(e1,e2)': 18,\n",
       "             'Member-Collection(e1,e2)': 19,\n",
       "             'Entity-Destination(e2,e1)': 20})"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TGT.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "if save_vocab_fname and verbose:\n",
    "    writeout = {\n",
    "        'tgt_vocab': {\n",
    "            'itos': TGT.vocab.itos, 'stoi': TGT.vocab.stoi,\n",
    "        },\n",
    "        'input_vocab': {\n",
    "            'itos': INPUT.vocab.itos, 'stoi': INPUT.vocab.stoi,\n",
    "        },\n",
    "    }\n",
    "    fwrite(json.dumps(writeout, indent=4), save_vocab_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⏰ Time: 10112040-18\t[Info] Finished building vocab: 24887 INPUT, 21 TGT\n"
     ]
    }
   ],
   "source": [
    "if verbose:\n",
    "    msg = \"[Info] Finished building vocab: {} INPUT, {} TGT\" \\\n",
    "        .format(len(INPUT.vocab), len(TGT.vocab))\n",
    "    show_time(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "proc_id=proc_id   #0\n",
    "n_gpus=n_gpus  # 1\n",
    "device=device  \n",
    "batch_size=args.batch_size   #10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _distribute_dataset(dataset):\n",
    "    n = len(dataset)\n",
    "    part = dataset[n * proc_id // n_gpus: n * (proc_id + 1) // n_gpus]\n",
    "    return torchtext.data.Dataset(part, dataset.fields)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = _distribute_dataset(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7200"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter, valid_iter = BucketIterator.splits(\n",
    "        (train_ds, valid_ds),\n",
    "        batch_sizes=(batch_size, batch_size),\n",
    "        sort_within_batch=True,\n",
    "        sort_key=lambda x: len(x.input),\n",
    "        device=device\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 10]\n",
       "\t[.tgt]:[torch.LongTensor of size 10x1]\n",
       "\t[.input]:[torch.LongTensor of size 10x19]\n",
       "\t[.show_inp]:['It is a controversial topic that provokes strong ENT_1_START arguments ENT_1_END for and against the ENT_2_START practice ENT_2_END .', \"Whenever we try a new butcher we always buy the ENT_1_START butcher ENT_1_END 's ENT_2_START sausage ENT_2_END first .\", 'The second ENT_1_START author ENT_1_END constructed the web ENT_2_START site ENT_2_END using a new open - source toolkit .', \"Vietnam 's response on the ENT_1_START toll ENT_1_END caused by the ENT_2_START earthquake ENT_2_END in Sichuan , China .\", 'Oh , so this was all about a ENT_1_START trash bag ENT_1_END with ENT_2_START money ENT_2_END in it .', 'The ENT_1_START assassination ENT_1_END resulted in extensive ENT_2_START arrests ENT_2_END of governmental , security , and criminal figures .', 'A superblack ENT_1_START material ENT_1_END made from microscopic carbon nanotubes is produced by a US ENT_2_START laboratory ENT_2_END .', 'The ENT_1_START speeches ENT_1_END were on the subject of the ENT_2_START relationship ENT_2_END between institutional and poetic violence .', 'The ENT_1_START mist ENT_1_END was carried into the ENT_2_START air ENT_2_END by bursting bubbles over the plating vats .', 'This ENT_1_START door ENT_1_END ENT_2_START handle ENT_2_END is in gold color and is installed on the front door .']"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item = next(iter(train_iter))\n",
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7],\n",
       "        [12],\n",
       "        [12],\n",
       "        [ 4],\n",
       "        [15],\n",
       "        [13],\n",
       "        [14],\n",
       "        [ 7],\n",
       "        [ 3],\n",
       "        [ 9]])"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 18])"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    4, 16430,     3,   359,    19,     9,   349,   483,     8,     6,\n",
       "            79,     5,     7],\n",
       "        [   12,     4,  5988,     3,    16,  1322,    23,     2,  5988,     6,\n",
       "           196,     5,     7],\n",
       "        [   12,  9173,  6580,     4,  4836,     3,    15,    52,    17,     6,\n",
       "          5721,     5,     7],\n",
       "        [   12,   626,     4,  1046,     3,   246,    23,     9,   579,     6,\n",
       "           706,     5,     7],\n",
       "        [   12,     4, 18239,     3,    16,  1136,    19,     9, 18200,     6,\n",
       "           625,     5,     7],\n",
       "        [ 8163,    10,     4,   731,     3,    11,   995,   103,   601,     6,\n",
       "          1654,     5,     7],\n",
       "        [  855,    15,     9,     4, 18803,     3,     8,   571,   951,     6,\n",
       "          3558,     5,     7],\n",
       "        [   12,     4,   358,     3,    15,   568,    17,     2,     6,   301,\n",
       "             5,     7,     1],\n",
       "        [  407,     4,   667,     3, 24691,     2,   327,    19,     6, 20881,\n",
       "             5,     7,     1],\n",
       "        [   70,    94,  5877,     4,  7680,     3,    14,    57,     6,  2843,\n",
       "             5,     7,     1]])"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'One ENT_1_START examination paper ENT_1_END was in ENT_2_START physical geography ENT_2_END , the other in political geography .'"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item.show_inp[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iter = BucketIterator(\n",
    "            test_ds,\n",
    "            batch_size=1,\n",
    "            sort=False,\n",
    "            sort_within_batch=False,\n",
    "            device=device\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 1]\n",
       "\t[.tgt]:[torch.LongTensor of size 1x1]\n",
       "\t[.input]:[torch.LongTensor of size 1x20]\n",
       "\t[.show_inp]:[\"Seasonal and nocturnal ENT_1_START migrations ENT_1_END cause sleep ENT_2_START deprivation ENT_2_END in diurnal Swainson 's thrush , Catharus ustulatus .\"]"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(test_iter))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 模型定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import LSTMClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(emb_vectors=dataset.INPUT.vocab.vectors,\n",
    "                           emb_dropout=args.emb_dropout,\n",
    "                           lstm_dim=args.lstm_dim,\n",
    "                           lstm_n_layer=args.lstm_n_layer,\n",
    "                           lstm_dropout=args.lstm_dropout,\n",
    "                           lstm_combine=args.lstm_combine,\n",
    "                           linear_dropout=args.linear_dropout,\n",
    "                           n_linear=args.n_linear,\n",
    "                           n_classes=len(dataset.TGT.vocab))    # vscde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_setup(proc_id, model, args):\n",
    "    def _count_parameters(model):\n",
    "        return sum(\n",
    "            p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    args.n_params = _count_parameters(model)\n",
    "\n",
    "    if proc_id == 0:\n",
    "        writeout = \" \".join(sys.argv[1:]).replace(' -', ' \\ \\n-')\n",
    "        writeout += '\\n' * 3 + \\\n",
    "                    json.dumps(args.__dict__, indent=4, sort_keys=True)\n",
    "        writeout += '\\n' * 3 + repr(model)\n",
    "\n",
    "        fwrite(writeout, args.save_meta_fname)\n",
    "\n",
    "        print('[Info] Model has {} trainable parameters'.format(args.n_params))\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Info] Model has 2652521 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "args = model_setup(proc_id, model, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding_layer): Embedding(24887, 100)\n",
       "  (embedding_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (lstm): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
       "  (lstm_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear_layers): ModuleList()\n",
       "  (linear_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (label): Linear(in_features=100, out_features=21, bias=True)\n",
       "  (crit): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 模型训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "from utils import shell, init_weights, set_seed\n",
    "\n",
    "from get_args import setup, model_setup, clean_up\n",
    "from dataloader import Dataset\n",
    "from evaluate import Validator, Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=args.epochs\n",
    "lr=args.lr\n",
    "weight_decay=args.weight_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adadelta(\n",
    "        filter(lambda p: p.requires_grad, model.parameters()), lr=1.0, rho=0.9,\n",
    "        eps=1e-6, weight_decay=weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding_layer): Embedding(24887, 100)\n",
       "  (embedding_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (lstm): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
       "  (lstm_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear_layers): ModuleList()\n",
       "  (linear_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (label): Linear(in_features=100, out_features=21, bias=True)\n",
       "  (crit): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss = 0\n",
    "n_correct = 0\n",
    "cnt = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/720 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "pbar = tqdm(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/720 [00:39<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "batch = next(iter(pbar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 7],\n",
       "        [ 2],\n",
       "        [14],\n",
       "        [ 4],\n",
       "        [ 8],\n",
       "        [ 9],\n",
       "        [ 2],\n",
       "        [ 4],\n",
       "        [ 3],\n",
       "        [ 2]])"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch.tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = len(batch.tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.loss_n_acc(batch.input, batch.tgt)   #vscode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_loss += loss.item() * batch_size\n",
    "cnt += batch_size\n",
    "n_correct += acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clip_gradient(model, clip_value):\n",
    "    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n",
    "    for p in params:\n",
    "        p.grad.data.clamp_(-clip_value, clip_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.zero_grad()\n",
    "loss.backward()\n",
    "clip_gradient(model, 1)\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5 模型评价\n",
    "* 4.5.1 验证\n",
    "* 4.5.2 最终测试"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.1 验证"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding_layer): Embedding(24887, 100)\n",
       "  (embedding_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (lstm): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
       "  (lstm_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear_layers): ModuleList()\n",
       "  (linear_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (label): Linear(in_features=100, out_features=21, bias=True)\n",
       "  (crit): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator = Validator(dataloader=valid_dl, save_dir=args.save_dir,\n",
    "                          save_log_fname=args.save_log_fname,\n",
    "                          save_model_fname=args.save_model_fname,\n",
    "                          valid_or_test='valid',\n",
    "                          vocab_itos=dataset.INPUT.vocab.itos,\n",
    "                          label_itos=dataset.TGT.vocab.itos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "validator.evaluate(model, 1)   # vscode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "validator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader=valid_dl\n",
    "save_dir=args.save_dir\n",
    "save_log_fname=args.save_log_fname\n",
    "save_model_fname=args.save_model_fname\n",
    "valid_or_test='valid'\n",
    "vocab_itos=dataset.INPUT.vocab.itos\n",
    "label_itos=dataset.TGT.vocab.itos\n",
    "best_loss = float('inf')\n",
    "best_epoch = 0\n",
    "epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = 0\n",
    "count = 0\n",
    "n_correct = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "[torchtext.data.batch.Batch of size 10]\n",
       "\t[.tgt]:[torch.LongTensor of size 10x1]\n",
       "\t[.input]:[torch.LongTensor of size 10x11]\n",
       "\t[.show_inp]:['The ENT_1_START carbon ENT_1_END sank into the ENT_2_START emitter ENT_2_END .', 'The ENT_1_START headmaster ENT_1_END made the formal ENT_2_START announcement ENT_2_END .', 'ENT_1_START Prospectors ENT_1_END have arrived in midland ENT_2_START cars ENT_2_END .', 'Its ENT_1_START introduction ENT_1_END supplies useful historical ENT_2_START background ENT_2_END .', 'ENT_1_START Biodiesel ENT_1_END is derived from vegetable ENT_2_START oils ENT_2_END .', 'Sometimes ENT_1_START joy ENT_1_END comes from unexpected ENT_2_START places ENT_2_END .', 'The ENT_1_START train ENT_1_END started to its ENT_2_START destination ENT_2_END .', 'The ENT_1_START party ENT_1_END starts in an ENT_2_START hour ENT_2_END .', 'ENT_1_START Rainwater ENT_1_END falls into special ENT_2_START use ENT_2_END .', 'The ENT_1_START nurse ENT_1_END wields the ENT_2_START scalpel ENT_2_END .']"
      ]
     },
     "execution_count": 213,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc = model.loss_n_acc(batch.input, batch.tgt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(3.0777, grad_fn=<NllLossBackward>)"
      ]
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "error += loss.item() * batch_size\n",
    "count += batch_size\n",
    "n_correct += acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_loss = (error / count)\n",
    "acc = (n_correct / count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "if (valid_or_test == 'valid') and (avg_loss < best_loss):\n",
    "            best_loss = avg_loss\n",
    "            best_epoch = epoch\n",
    "\n",
    "            checkpoint = {\n",
    "                'model': model.state_dict(),\n",
    "                'model_opt': model.opts,\n",
    "                'epoch': epoch,\n",
    "            }\n",
    "            torch.save(checkpoint, save_model_fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.5.2 最终测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = Predictor(args.save_vocab_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.use_pretrained_model(args.save_model_fname, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tmp/model'"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.save_model_fname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = torch.load(args.save_model_fname)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': OrderedDict([('att_w',\n",
       "               tensor([[[ 2.1291],\n",
       "                        [ 0.3747],\n",
       "                        [ 0.3253],\n",
       "                        [-1.5362],\n",
       "                        [ 0.8928],\n",
       "                        [ 0.1631],\n",
       "                        [-0.1253],\n",
       "                        [ 0.8173],\n",
       "                        [-0.5952],\n",
       "                        [-0.5248],\n",
       "                        [ 0.5028],\n",
       "                        [ 0.6665],\n",
       "                        [ 0.4987],\n",
       "                        [-0.1424],\n",
       "                        [ 0.5954],\n",
       "                        [ 0.0886],\n",
       "                        [-0.7207],\n",
       "                        [-1.4731],\n",
       "                        [-0.6029],\n",
       "                        [ 0.9682],\n",
       "                        [ 0.6790],\n",
       "                        [ 0.1617],\n",
       "                        [ 0.8173],\n",
       "                        [-0.2477],\n",
       "                        [-0.0329],\n",
       "                        [ 0.5171],\n",
       "                        [ 2.5996],\n",
       "                        [ 0.4053],\n",
       "                        [-0.5590],\n",
       "                        [-0.9657],\n",
       "                        [-0.4814],\n",
       "                        [ 0.7554],\n",
       "                        [-0.3652],\n",
       "                        [ 0.2567],\n",
       "                        [-0.1441],\n",
       "                        [-0.6041],\n",
       "                        [-1.6748],\n",
       "                        [-0.3716],\n",
       "                        [-0.0135],\n",
       "                        [ 0.1153],\n",
       "                        [-0.5217],\n",
       "                        [-2.4657],\n",
       "                        [-0.5704],\n",
       "                        [ 0.2629],\n",
       "                        [-1.3265],\n",
       "                        [-0.3957],\n",
       "                        [ 0.8601],\n",
       "                        [ 0.0175],\n",
       "                        [-0.0184],\n",
       "                        [ 0.8194],\n",
       "                        [-0.0248],\n",
       "                        [-1.0660],\n",
       "                        [-0.4829],\n",
       "                        [ 1.6808],\n",
       "                        [-0.3513],\n",
       "                        [-0.5174],\n",
       "                        [-2.1224],\n",
       "                        [ 0.8339],\n",
       "                        [ 0.0591],\n",
       "                        [ 0.6021],\n",
       "                        [ 0.4939],\n",
       "                        [ 0.5525],\n",
       "                        [-2.2555],\n",
       "                        [ 1.7580],\n",
       "                        [ 0.2704],\n",
       "                        [-0.8779],\n",
       "                        [-0.2218],\n",
       "                        [-0.9767],\n",
       "                        [ 0.8210],\n",
       "                        [-0.9720],\n",
       "                        [-0.0419],\n",
       "                        [ 1.6232],\n",
       "                        [-1.0411],\n",
       "                        [-1.0811],\n",
       "                        [-1.0135],\n",
       "                        [-0.0878],\n",
       "                        [-0.2122],\n",
       "                        [ 0.8305],\n",
       "                        [ 1.7862],\n",
       "                        [ 0.6848],\n",
       "                        [-0.5772],\n",
       "                        [ 0.2704],\n",
       "                        [-1.6642],\n",
       "                        [ 0.5160],\n",
       "                        [ 0.5265],\n",
       "                        [-1.2406],\n",
       "                        [ 0.2836],\n",
       "                        [-1.1159],\n",
       "                        [-0.2250],\n",
       "                        [-0.6741],\n",
       "                        [-1.7720],\n",
       "                        [-0.1500],\n",
       "                        [ 0.1162],\n",
       "                        [ 1.0745],\n",
       "                        [-0.1079],\n",
       "                        [-0.3738],\n",
       "                        [-0.1755],\n",
       "                        [ 1.1088],\n",
       "                        [-0.8904],\n",
       "                        [-0.3895]]])),\n",
       "              ('embedding_layer.weight',\n",
       "               tensor([[-0.4743, -0.4428, -1.7312,  ..., -0.3258, -1.7770, -1.4064],\n",
       "                       [ 0.1588, -0.2377, -0.1367,  ...,  0.3012,  0.5905, -2.3491],\n",
       "                       [ 0.0867, -2.2392,  0.9784,  ...,  2.7481, -1.0433, -0.3884],\n",
       "                       ...,\n",
       "                       [ 1.4081,  1.5455,  0.5430,  ...,  1.1899,  1.3553, -1.6993],\n",
       "                       [-0.2735, -1.6369,  0.4976,  ..., -0.2575, -1.0777, -0.0204],\n",
       "                       [-0.7505, -1.3088,  0.6718,  ...,  0.5122, -0.5507, -0.8585]])),\n",
       "              ('lstm.weight_ih_l0',\n",
       "               tensor([[ 0.0842,  0.0712,  0.0764,  ..., -0.0526, -0.0426, -0.0277],\n",
       "                       [ 0.0306, -0.0420,  0.0318,  ..., -0.0969,  0.0724, -0.0868],\n",
       "                       [-0.0458,  0.0587,  0.0536,  ...,  0.0625, -0.0460,  0.0726],\n",
       "                       ...,\n",
       "                       [ 0.0265,  0.0035,  0.0032,  ..., -0.0102,  0.0128,  0.0614],\n",
       "                       [ 0.0475, -0.0681,  0.0265,  ...,  0.0513, -0.0490,  0.0877],\n",
       "                       [ 0.0239, -0.0224,  0.0097,  ...,  0.0089,  0.0896, -0.0971]])),\n",
       "              ('lstm.weight_hh_l0',\n",
       "               tensor([[ 0.0238, -0.0391, -0.0709,  ..., -0.0138,  0.0118, -0.0009],\n",
       "                       [-0.0574,  0.0335,  0.0650,  ..., -0.0406, -0.0613,  0.0034],\n",
       "                       [-0.0006,  0.0315,  0.0435,  ...,  0.0188, -0.0773,  0.0769],\n",
       "                       ...,\n",
       "                       [-0.0887, -0.0490,  0.0891,  ..., -0.0434,  0.0694, -0.0578],\n",
       "                       [-0.0146,  0.0996,  0.0442,  ...,  0.0883,  0.0701,  0.0517],\n",
       "                       [-0.0314, -0.0695,  0.0154,  ..., -0.0227,  0.0254, -0.0649]])),\n",
       "              ('lstm.bias_ih_l0',\n",
       "               tensor([ 0.0394,  0.0063, -0.0290, -0.0311,  0.0707,  0.0203,  0.0881, -0.0686,\n",
       "                       -0.0061, -0.0148, -0.0969, -0.0884,  0.0655, -0.0751, -0.0912,  0.0483,\n",
       "                       -0.0450, -0.0933,  0.0702, -0.0366,  0.0091,  0.0785,  0.0341, -0.0450,\n",
       "                        0.0749,  0.0648, -0.0991,  0.0650,  0.0657,  0.0579,  0.0518,  0.0808,\n",
       "                        0.0407, -0.0143, -0.0799, -0.0441,  0.0378,  0.0875, -0.0251,  0.0570,\n",
       "                        0.0223, -0.0361, -0.0043,  0.0059,  0.0876,  0.0160,  0.0018,  0.0606,\n",
       "                       -0.0037,  0.0483,  0.0528,  0.0315,  0.0134, -0.0731, -0.0414, -0.0056,\n",
       "                        0.0214,  0.0695,  0.0025, -0.0403, -0.0228, -0.0584,  0.0227, -0.0634,\n",
       "                        0.0573, -0.0998, -0.0371,  0.0663,  0.0911, -0.0232, -0.0144, -0.0964,\n",
       "                        0.0106,  0.0095, -0.0399, -0.0964,  0.0976,  0.0280,  0.0183,  0.0985,\n",
       "                       -0.0082,  0.0899, -0.0991, -0.0742, -0.0950,  0.0860, -0.0860,  0.0416,\n",
       "                        0.0238,  0.0435,  0.0289,  0.0354,  0.0764,  0.0881,  0.0857,  0.0903,\n",
       "                        0.0702, -0.0880, -0.0329, -0.0033,  0.0398,  0.0852, -0.0420, -0.0221,\n",
       "                       -0.0492,  0.0955,  0.0622, -0.0291,  0.0164,  0.0388, -0.0727,  0.0593,\n",
       "                        0.0505,  0.0721, -0.0136,  0.0183, -0.0329,  0.0979, -0.0141,  0.0688,\n",
       "                        0.0628, -0.0847,  0.0118, -0.0247, -0.0314,  0.0408, -0.0508, -0.0661,\n",
       "                       -0.0240,  0.0035,  0.0379, -0.0142,  0.0214, -0.0059,  0.0312,  0.0667,\n",
       "                       -0.0123,  0.0570,  0.0341,  0.0201, -0.0168, -0.0342, -0.0460, -0.0605,\n",
       "                       -0.0598,  0.0744, -0.0385,  0.0951,  0.0751,  0.0676, -0.0579, -0.0379,\n",
       "                       -0.0823,  0.0954, -0.0543, -0.0714,  0.0951, -0.0811,  0.0023,  0.0142,\n",
       "                       -0.0251, -0.0666,  0.0580,  0.0978,  0.0437, -0.0949, -0.0514,  0.0991,\n",
       "                        0.0671,  0.0020,  0.0555, -0.0041,  0.0478,  0.0071,  0.0548,  0.0643,\n",
       "                        0.0504,  0.0782,  0.0432,  0.0347,  0.0359,  0.0349,  0.0562, -0.0050,\n",
       "                       -0.0753, -0.0169, -0.0092, -0.0565, -0.0521, -0.0360,  0.0122, -0.0641,\n",
       "                        0.0866, -0.0842, -0.0688, -0.0880, -0.0974, -0.0322, -0.0248, -0.0445,\n",
       "                        0.0089, -0.0619,  0.0132,  0.0806, -0.0922,  0.0600, -0.0393, -0.0802,\n",
       "                       -0.0054,  0.0102, -0.0021, -0.0774, -0.0143, -0.0993, -0.0085,  0.0939,\n",
       "                        0.0074, -0.0542, -0.0489, -0.0855, -0.0609,  0.0675, -0.0794,  0.0968,\n",
       "                       -0.0941,  0.0935, -0.0770,  0.0961, -0.0797, -0.0859, -0.0021,  0.0068,\n",
       "                       -0.0373, -0.0927,  0.0034,  0.0667,  0.0719, -0.0303, -0.0795,  0.0602,\n",
       "                       -0.0326, -0.0700, -0.0628, -0.0265, -0.0460,  0.1001,  0.0572,  0.0454,\n",
       "                       -0.0247,  0.0835,  0.0048,  0.0305, -0.0351,  0.0484, -0.0666,  0.0523,\n",
       "                       -0.0864, -0.0195, -0.0059, -0.0057,  0.0781, -0.0345,  0.0165,  0.0057,\n",
       "                       -0.0366, -0.0366, -0.0161,  0.0796, -0.0591,  0.0442,  0.0779, -0.0639,\n",
       "                        0.0975, -0.0435,  0.0172, -0.0911, -0.0895,  0.0675,  0.0258, -0.0772,\n",
       "                        0.0795,  0.0269,  0.0064, -0.0492,  0.0394,  0.0956, -0.0697, -0.0584,\n",
       "                        0.0873,  0.0879, -0.0818, -0.0292, -0.0936, -0.0246, -0.0269,  0.0747,\n",
       "                       -0.0847, -0.0520, -0.0362, -0.0325, -0.0522,  0.0916, -0.0630,  0.0417,\n",
       "                       -0.0752, -0.0032,  0.0303,  0.0836, -0.0341,  0.0591,  0.0839,  0.0346,\n",
       "                        0.0974, -0.0845,  0.0782,  0.0088, -0.0789,  0.0624, -0.0508,  0.0115,\n",
       "                        0.0759,  0.0049,  0.0598,  0.0837, -0.0701, -0.0204,  0.0899, -0.0061,\n",
       "                       -0.0693,  0.0422, -0.0566,  0.0555, -0.0290, -0.0776,  0.0219, -0.0274,\n",
       "                       -0.0676, -0.0540,  0.0369, -0.0778,  0.0764,  0.0228,  0.0576, -0.0388,\n",
       "                       -0.0388, -0.0176, -0.0144,  0.0101, -0.0431,  0.0595,  0.0943,  0.0602,\n",
       "                        0.0239, -0.0962,  0.0954,  0.0909,  0.0739,  0.0529, -0.0550,  0.0402,\n",
       "                        0.0408, -0.0031, -0.0872, -0.0889,  0.0212,  0.0366, -0.0004, -0.0277,\n",
       "                        0.0872,  0.0110, -0.0649, -0.0255,  0.0888,  0.0258,  0.0851, -0.0099,\n",
       "                       -0.0256, -0.0244, -0.0156,  0.0823, -0.0317, -0.0651,  0.0408, -0.0516,\n",
       "                        0.0741, -0.0356, -0.0562, -0.0227, -0.0673,  0.0593, -0.0535,  0.0599,\n",
       "                        0.0096, -0.0615,  0.0376,  0.0230, -0.0948, -0.0659,  0.0509, -0.0823])),\n",
       "              ('lstm.bias_hh_l0',\n",
       "               tensor([-0.0914,  0.0084,  0.0161, -0.0219,  0.0552, -0.0581, -0.0931,  0.0709,\n",
       "                        0.0624,  0.0237,  0.0498, -0.0317, -0.0166, -0.0154,  0.0672,  0.0622,\n",
       "                        0.0346, -0.0475,  0.0749,  0.0342, -0.0101,  0.0623, -0.0567, -0.0506,\n",
       "                       -0.0341, -0.0322,  0.0337, -0.0661,  0.0095,  0.0707,  0.0345, -0.0818,\n",
       "                       -0.0261, -0.0021,  0.0693, -0.0151, -0.0328, -0.0921,  0.0527,  0.0511,\n",
       "                       -0.0416,  0.0494, -0.0422, -0.0856, -0.0870, -0.0367,  0.0916, -0.0233,\n",
       "                       -0.0038,  0.0215,  0.0248,  0.0509, -0.0019,  0.0245, -0.0510,  0.0181,\n",
       "                        0.0161,  0.0064,  0.0065,  0.0924, -0.0729, -0.0202, -0.0210,  0.0606,\n",
       "                       -0.0163, -0.0151,  0.0340,  0.0745, -0.0543,  0.0375,  0.0290,  0.0459,\n",
       "                        0.0156, -0.0968,  0.0071,  0.0539, -0.0177,  0.0320,  0.0148,  0.0203,\n",
       "                        0.0059, -0.0841,  0.0047, -0.0792,  0.0766,  0.0909, -0.0831,  0.0723,\n",
       "                       -0.0667,  0.0390, -0.0605, -0.0378, -0.0633,  0.0422, -0.0699,  0.0774,\n",
       "                       -0.0668, -0.0598, -0.0715,  0.0029,  0.0079, -0.0470,  0.0795, -0.0451,\n",
       "                       -0.0629, -0.0336, -0.0526,  0.0681,  0.0029,  0.0187, -0.0352,  0.0137,\n",
       "                       -0.0637, -0.0072, -0.0094, -0.0425, -0.0912, -0.0714,  0.0773, -0.0363,\n",
       "                       -0.0800,  0.0561, -0.0421, -0.0074,  0.0269, -0.0418,  0.0369, -0.0250,\n",
       "                        0.0666, -0.0567, -0.0224,  0.0310, -0.0830,  0.0209,  0.0260,  0.0083,\n",
       "                        0.0145,  0.0791, -0.0347, -0.0872, -0.0638,  0.0551,  0.0247,  0.0574,\n",
       "                        0.0741, -0.0430,  0.0246,  0.0932, -0.0279, -0.0488,  0.0302,  0.0236,\n",
       "                       -0.0226,  0.0401, -0.0423, -0.0282,  0.0889, -0.0758,  0.0085, -0.0461,\n",
       "                       -0.0010,  0.0710,  0.0959,  0.0577, -0.0584,  0.0813, -0.0292, -0.0268,\n",
       "                       -0.0913,  0.0541,  0.0366, -0.0312, -0.0291,  0.0850,  0.0229, -0.0253,\n",
       "                        0.0209,  0.0990, -0.0577, -0.0863,  0.0897,  0.0185, -0.0480,  0.0655,\n",
       "                        0.0712, -0.0079, -0.0035, -0.0331,  0.0850, -0.0580, -0.0240,  0.0870,\n",
       "                        0.0285, -0.0237, -0.0095,  0.0950,  0.0923, -0.0401, -0.0875, -0.0883,\n",
       "                       -0.0511,  0.0513, -0.0710,  0.0847, -0.0950, -0.0745, -0.0248, -0.0815,\n",
       "                       -0.0216,  0.0016, -0.0566,  0.0085,  0.0919, -0.0232, -0.0471, -0.0088,\n",
       "                       -0.0543, -0.0076,  0.0428, -0.0164, -0.0381,  0.0899, -0.0234, -0.0342,\n",
       "                        0.0970, -0.0612, -0.0076, -0.0008,  0.0711, -0.0573, -0.1020, -0.0291,\n",
       "                       -0.0679, -0.0121,  0.0145,  0.0623, -0.0039, -0.0530, -0.0034,  0.0534,\n",
       "                       -0.0918,  0.0568, -0.0726, -0.0664, -0.0802, -0.0641, -0.0200,  0.0100,\n",
       "                       -0.0856, -0.0351,  0.0713, -0.0214,  0.0776, -0.0945, -0.0428, -0.0431,\n",
       "                        0.0620,  0.0752, -0.0943,  0.0299, -0.0711, -0.0162, -0.0086,  0.0491,\n",
       "                        0.0953,  0.0570, -0.0224, -0.0719,  0.0159,  0.0611, -0.0965,  0.0378,\n",
       "                       -0.0732,  0.0574,  0.0525,  0.0861, -0.0910,  0.0138,  0.0101,  0.1001,\n",
       "                        0.0979, -0.0691, -0.0012,  0.0215,  0.0450,  0.0860, -0.0352,  0.0394,\n",
       "                        0.0776, -0.0430,  0.0813,  0.0155,  0.0675, -0.0803, -0.0805,  0.0820,\n",
       "                       -0.0548,  0.0132,  0.0602,  0.0099,  0.0032,  0.0527,  0.0922, -0.0161,\n",
       "                        0.0789,  0.0668, -0.0619, -0.0240,  0.0229,  0.0833, -0.0675,  0.0414,\n",
       "                        0.0922, -0.0441,  0.0404,  0.0267,  0.0015,  0.0928, -0.0802,  0.0043,\n",
       "                       -0.0158,  0.0857,  0.0699,  0.0311,  0.0800, -0.0204, -0.0770,  0.0230,\n",
       "                        0.0557, -0.0816, -0.0345,  0.0507,  0.0421, -0.0503, -0.0537, -0.0293,\n",
       "                       -0.0400,  0.0083,  0.0651, -0.0006,  0.0042,  0.0204,  0.0697,  0.0783,\n",
       "                       -0.0853, -0.0804, -0.0462,  0.0018,  0.0811, -0.0685,  0.0342,  0.0646,\n",
       "                       -0.0196, -0.0681, -0.0432, -0.0359,  0.0881,  0.0776,  0.0462,  0.0955,\n",
       "                        0.0908,  0.0391, -0.0711,  0.0725, -0.0422, -0.0043, -0.0257, -0.0487,\n",
       "                       -0.0524,  0.0231, -0.0555,  0.0826, -0.0243,  0.0022, -0.0138,  0.0592,\n",
       "                       -0.0635, -0.0741,  0.0809, -0.0497,  0.0197, -0.0191, -0.0881,  0.0452,\n",
       "                        0.0020, -0.0333,  0.0597,  0.0319, -0.0456, -0.0865,  0.0773,  0.0032,\n",
       "                       -0.0213,  0.0879, -0.0598,  0.0880,  0.0295,  0.0705, -0.0889,  0.0757])),\n",
       "              ('lstm.weight_ih_l0_reverse',\n",
       "               tensor([[ 0.0004,  0.0492,  0.0625,  ...,  0.0196,  0.0235, -0.0441],\n",
       "                       [ 0.0640, -0.0572, -0.0807,  ..., -0.0591, -0.0947,  0.0253],\n",
       "                       [-0.0014, -0.0171, -0.0933,  ..., -0.0277, -0.0234, -0.0825],\n",
       "                       ...,\n",
       "                       [ 0.0653,  0.0575,  0.0317,  ...,  0.0076, -0.1001, -0.0954],\n",
       "                       [ 0.0014, -0.0611,  0.0483,  ...,  0.0461, -0.0027,  0.0074],\n",
       "                       [-0.0629,  0.0375, -0.0749,  ..., -0.0527, -0.0981,  0.0812]])),\n",
       "              ('lstm.weight_hh_l0_reverse',\n",
       "               tensor([[-0.0098, -0.0515, -0.0329,  ...,  0.0871, -0.0220,  0.0372],\n",
       "                       [-0.0326, -0.0406, -0.0024,  ...,  0.0521,  0.0355, -0.0364],\n",
       "                       [ 0.0794,  0.0501, -0.0319,  ...,  0.0474,  0.0189,  0.0261],\n",
       "                       ...,\n",
       "                       [-0.0925,  0.0477, -0.0736,  ...,  0.0743,  0.0946, -0.0207],\n",
       "                       [ 0.0625,  0.0854,  0.0580,  ...,  0.0765, -0.0892, -0.0437],\n",
       "                       [ 0.0238, -0.0320, -0.0418,  ...,  0.0319,  0.0388,  0.0860]])),\n",
       "              ('lstm.bias_ih_l0_reverse',\n",
       "               tensor([-9.6354e-02, -5.4303e-02, -2.1649e-02, -8.8650e-02,  2.3229e-03,\n",
       "                        1.5532e-02, -7.8898e-02, -3.6932e-02,  9.8557e-02, -7.9694e-03,\n",
       "                        9.8303e-02,  7.0021e-02, -9.6053e-02, -8.0448e-02,  4.3447e-02,\n",
       "                        4.0363e-02,  4.4235e-02, -9.7201e-02, -6.7551e-02, -8.6320e-02,\n",
       "                       -2.4318e-02, -7.7035e-02, -9.4780e-02, -6.4904e-02,  5.2863e-02,\n",
       "                        8.7169e-02, -7.3567e-02, -3.2810e-02, -3.3800e-02, -9.6386e-02,\n",
       "                        8.3676e-02, -1.5877e-02,  4.1360e-02, -1.7226e-02, -5.8167e-03,\n",
       "                        7.6971e-02, -4.9423e-02, -4.0951e-02, -2.7950e-02,  7.1106e-02,\n",
       "                       -2.9501e-02, -1.4449e-02, -7.2296e-02,  3.2617e-02, -9.6109e-02,\n",
       "                        8.6948e-03,  9.0030e-03, -1.4842e-02,  7.1340e-02,  4.7406e-02,\n",
       "                        3.6962e-02,  3.9849e-02, -8.5354e-02,  9.4761e-02, -2.7698e-02,\n",
       "                        1.3790e-02, -7.3667e-02, -9.4621e-02,  2.0512e-02,  6.9346e-02,\n",
       "                       -4.5047e-02, -4.8300e-02,  8.7459e-02, -7.0539e-02, -5.7550e-03,\n",
       "                        5.4993e-02, -5.7107e-02,  1.8685e-02, -6.2075e-02, -4.8020e-02,\n",
       "                        4.5896e-02, -3.0111e-02, -4.5279e-02,  7.5903e-02,  7.3353e-02,\n",
       "                        2.4903e-02,  2.0884e-02, -8.6412e-02,  9.0075e-02, -3.7980e-02,\n",
       "                       -4.5505e-02,  7.5744e-02, -7.8369e-02,  9.7793e-02, -9.6608e-02,\n",
       "                       -6.4318e-02, -5.6609e-02,  7.0262e-02, -9.1871e-02, -9.1894e-02,\n",
       "                        4.0544e-02, -5.2284e-02, -9.1187e-02,  3.4287e-02,  7.0624e-02,\n",
       "                       -5.6629e-03,  7.6601e-02, -4.3323e-02,  4.2543e-02,  8.6090e-02,\n",
       "                        5.5180e-02, -1.9796e-02,  6.1630e-02,  4.1833e-02, -6.6651e-02,\n",
       "                       -8.2597e-02,  8.6927e-02,  4.3658e-03,  8.0809e-02,  4.4767e-02,\n",
       "                        1.9192e-02,  6.5999e-02, -6.4173e-02, -7.6978e-02, -2.5530e-02,\n",
       "                       -6.5692e-02,  6.9923e-02, -3.6347e-02,  7.9117e-02,  6.2730e-02,\n",
       "                       -7.3536e-02, -7.8271e-02,  4.1611e-02, -3.1794e-02,  6.7865e-02,\n",
       "                       -8.1349e-02, -7.5239e-02,  8.9838e-02, -1.1238e-02,  6.5489e-02,\n",
       "                       -7.2883e-02, -2.8666e-02, -8.1644e-02,  4.8891e-02, -3.5722e-02,\n",
       "                        1.1890e-02,  9.6763e-02, -5.5153e-02, -8.5414e-02,  5.2944e-02,\n",
       "                       -8.3875e-03,  5.6926e-02, -3.4788e-02,  4.7979e-02,  1.3819e-02,\n",
       "                       -7.0537e-02,  6.9388e-02, -6.5547e-02, -1.1594e-02,  8.3386e-02,\n",
       "                        1.6251e-02,  9.5418e-02, -7.8900e-02,  5.4795e-03,  1.3544e-02,\n",
       "                        6.0094e-02,  3.5555e-02,  1.5982e-02,  8.4169e-02,  5.2097e-02,\n",
       "                        1.3519e-02, -2.1208e-03,  7.4688e-02,  3.0131e-02,  1.0927e-02,\n",
       "                       -5.5131e-02,  5.3933e-02,  4.1120e-02,  2.1327e-02,  9.6428e-02,\n",
       "                       -7.5364e-02,  8.9799e-02, -9.2183e-02, -8.7656e-02, -8.7013e-02,\n",
       "                       -9.0962e-02, -7.5360e-02,  4.5132e-03, -4.5561e-02, -5.8599e-02,\n",
       "                       -2.4784e-02, -3.3731e-02,  2.1636e-02,  9.9158e-02,  6.1541e-02,\n",
       "                       -8.6398e-02,  8.9949e-02, -4.7274e-02,  2.3301e-02, -5.6344e-02,\n",
       "                        2.5740e-02, -9.5293e-02, -4.7217e-03,  4.2090e-02,  5.5065e-02,\n",
       "                       -5.8237e-03,  7.0434e-02, -2.3237e-02, -8.6715e-02,  1.9287e-02,\n",
       "                       -3.4961e-02, -7.4716e-02,  9.5911e-02,  2.6262e-03, -4.2866e-02,\n",
       "                        1.9024e-02,  1.4300e-02, -4.8990e-02, -2.7202e-02,  2.3151e-02,\n",
       "                        5.5386e-03, -4.0589e-02, -4.8105e-02, -6.9906e-02,  1.8454e-02,\n",
       "                       -5.7415e-02, -1.0696e-02, -3.4098e-02,  2.8191e-02, -7.2003e-02,\n",
       "                       -8.3977e-02,  9.9170e-04,  2.2644e-03, -7.1068e-02, -5.3610e-03,\n",
       "                        4.5671e-02, -9.3296e-02, -5.3405e-02,  3.5020e-02, -4.9445e-02,\n",
       "                       -4.6206e-02, -6.0327e-02,  1.8382e-02, -1.6791e-02,  2.4027e-02,\n",
       "                       -2.4793e-02,  3.1709e-02,  4.6036e-02,  4.0405e-03, -1.0129e-01,\n",
       "                        3.9570e-02, -8.2028e-02,  2.9910e-03,  8.4349e-02,  5.6988e-02,\n",
       "                       -9.2463e-02, -5.3160e-02, -9.1275e-02, -2.7075e-02,  2.9273e-02,\n",
       "                       -2.1808e-02, -4.2695e-02, -4.9005e-02,  6.5732e-02,  1.5629e-02,\n",
       "                        2.9091e-02, -2.9091e-02, -7.6604e-02,  4.2829e-02, -5.9427e-02,\n",
       "                       -4.6122e-02,  3.1549e-02, -1.3193e-02,  8.5300e-02,  7.9957e-02,\n",
       "                        5.5133e-02,  8.7444e-02,  9.4028e-02,  6.7194e-02, -9.5459e-02,\n",
       "                        2.9793e-02, -6.2188e-02,  7.2250e-02, -3.1888e-02,  1.4995e-03,\n",
       "                        7.6690e-02,  5.0797e-02,  5.3704e-02, -1.6731e-02, -5.9111e-02,\n",
       "                        2.5757e-03,  8.2735e-02,  3.3074e-02, -4.5669e-02,  6.0432e-02,\n",
       "                        9.4407e-02, -4.7082e-02,  1.7692e-02, -2.4870e-02, -3.8394e-02,\n",
       "                        7.2863e-02, -8.2359e-02,  1.9102e-03,  6.5923e-03,  9.0973e-02,\n",
       "                       -6.7638e-02, -6.8968e-02,  4.4783e-02,  9.6692e-02,  2.6577e-02,\n",
       "                        5.8955e-02, -2.6089e-02, -5.1815e-02,  9.9016e-03, -9.3339e-02,\n",
       "                        2.9637e-02, -7.4544e-02, -9.1895e-02, -6.1747e-02, -9.5946e-02,\n",
       "                       -2.8485e-02, -7.3087e-02,  8.7052e-03,  6.4420e-02, -3.1101e-02,\n",
       "                        6.0053e-02,  6.6442e-03, -6.7854e-02, -2.3612e-03,  9.9528e-02,\n",
       "                        5.7149e-02, -3.4322e-02,  8.7472e-02, -4.7756e-05,  1.0980e-02,\n",
       "                        8.9660e-02,  1.8028e-03, -7.1351e-02, -4.6775e-02,  9.2487e-03,\n",
       "                       -5.8313e-02,  6.0555e-02, -2.0026e-02,  9.5057e-02,  3.4344e-02,\n",
       "                       -2.5406e-02, -9.6846e-02, -3.2946e-02,  7.6101e-02, -8.8328e-02,\n",
       "                        8.0797e-02, -6.9127e-03,  6.7710e-02, -8.9581e-02, -6.7699e-02,\n",
       "                        2.4711e-02,  3.8086e-02,  6.7410e-02, -1.1249e-02, -2.0056e-02,\n",
       "                       -3.1496e-02,  4.2406e-02, -1.4787e-02, -1.7595e-02, -6.6105e-02,\n",
       "                        9.1632e-02,  6.3565e-02,  4.5240e-02,  6.7331e-02, -3.9551e-02,\n",
       "                        2.3897e-03,  2.2348e-02, -9.4758e-02, -3.4287e-02, -4.4865e-02,\n",
       "                       -9.3926e-02,  4.6174e-02, -5.5550e-02,  1.3627e-03,  3.8683e-02,\n",
       "                        1.7617e-02, -4.0734e-02, -3.2455e-02,  3.5271e-02,  8.3597e-02,\n",
       "                       -8.5529e-02, -3.1311e-02,  3.8367e-02, -1.5901e-02,  2.0018e-02,\n",
       "                       -9.2133e-02, -7.8138e-02, -1.0102e-01,  2.8311e-02, -8.7360e-02,\n",
       "                        3.6199e-02, -7.7823e-02, -3.4976e-02, -1.3381e-03, -9.2081e-03,\n",
       "                        6.6462e-03,  2.6891e-02,  5.2321e-02,  3.2193e-02,  1.6147e-02,\n",
       "                        5.3815e-02,  2.4024e-02,  2.4692e-02, -7.8166e-02,  3.7304e-03])),\n",
       "              ('lstm.bias_hh_l0_reverse',\n",
       "               tensor([ 0.0133, -0.0421,  0.0014,  0.0110,  0.0022, -0.0709, -0.0452,  0.0409,\n",
       "                       -0.0481,  0.0592, -0.0149,  0.0311,  0.0288, -0.0766, -0.0672,  0.0566,\n",
       "                       -0.0569, -0.0854,  0.0416,  0.0179,  0.0276,  0.0263, -0.0674,  0.0614,\n",
       "                        0.0802, -0.0297, -0.0406, -0.0098,  0.0736, -0.0062,  0.0901,  0.0073,\n",
       "                       -0.0955,  0.0259, -0.0325, -0.0639,  0.0498,  0.0084,  0.0652, -0.0244,\n",
       "                       -0.0685,  0.0808,  0.0813, -0.0170, -0.0635, -0.0921, -0.0812,  0.0662,\n",
       "                        0.0800,  0.0563, -0.0906,  0.0188,  0.0062, -0.0867, -0.0357,  0.0167,\n",
       "                       -0.0965, -0.0237,  0.0388,  0.0074, -0.0360, -0.0487,  0.0887,  0.0864,\n",
       "                        0.0148, -0.0054, -0.0661,  0.0400, -0.0532,  0.0744,  0.0476, -0.0123,\n",
       "                       -0.0971, -0.0001, -0.0272, -0.0269,  0.0968, -0.0467,  0.0105,  0.0767,\n",
       "                        0.0054,  0.0148,  0.0645, -0.0924, -0.0202, -0.0285,  0.0029,  0.0102,\n",
       "                       -0.0981,  0.0754, -0.0910,  0.0607, -0.0679, -0.0535, -0.0187, -0.0375,\n",
       "                       -0.0563, -0.0082,  0.0461,  0.0298,  0.0911, -0.0766, -0.0431, -0.0875,\n",
       "                        0.0524, -0.0097,  0.0778,  0.0207,  0.0122,  0.0905, -0.0467,  0.0897,\n",
       "                        0.0126, -0.0030, -0.0672,  0.0498,  0.0270, -0.0686,  0.0376,  0.0472,\n",
       "                       -0.0621,  0.0040,  0.0494, -0.0199,  0.0652,  0.0551, -0.0167,  0.0732,\n",
       "                       -0.0056,  0.0146,  0.0195, -0.0589,  0.0779, -0.0009,  0.0594, -0.0098,\n",
       "                        0.0882,  0.0616,  0.0495,  0.0602,  0.0516, -0.0006, -0.0577,  0.0894,\n",
       "                        0.0721, -0.0093, -0.0026,  0.0569,  0.0604,  0.0774,  0.0394, -0.0903,\n",
       "                       -0.0561, -0.0963,  0.0711,  0.0754,  0.0220, -0.0709,  0.0977,  0.0016,\n",
       "                        0.0477,  0.0494, -0.0779, -0.0714,  0.0408, -0.0337,  0.0087, -0.0497,\n",
       "                        0.0547, -0.0903,  0.0466, -0.0272,  0.0692, -0.0119,  0.0264, -0.0619,\n",
       "                       -0.0915,  0.0227, -0.0287, -0.0230, -0.0105,  0.0008,  0.0955,  0.0058,\n",
       "                       -0.0913, -0.0643,  0.0156, -0.0150, -0.0934, -0.0658,  0.0115, -0.0579,\n",
       "                        0.0901,  0.0128, -0.0516,  0.0130,  0.0472,  0.0727, -0.0133,  0.0086,\n",
       "                       -0.0172,  0.0254,  0.0509,  0.0019, -0.0818,  0.0171,  0.0512,  0.0414,\n",
       "                        0.0549, -0.0901,  0.0071,  0.0527, -0.0710, -0.0842, -0.0662, -0.0936,\n",
       "                        0.0966,  0.0857, -0.0106, -0.0715,  0.0663, -0.0219,  0.0266, -0.0927,\n",
       "                       -0.0128, -0.0780,  0.0739, -0.0900,  0.0385, -0.0504,  0.0790,  0.0170,\n",
       "                       -0.0220,  0.0908,  0.0635,  0.0803,  0.0541,  0.0457, -0.0241, -0.0301,\n",
       "                       -0.0481, -0.0325, -0.0982,  0.0862, -0.0478,  0.0086,  0.0209,  0.0371,\n",
       "                        0.0920,  0.0940,  0.0134, -0.0793,  0.0716,  0.0330, -0.0342,  0.0427,\n",
       "                        0.0756,  0.0108,  0.0745,  0.0639, -0.0984, -0.0300, -0.0530, -0.0298,\n",
       "                        0.0351, -0.0735,  0.0765, -0.0905, -0.0039,  0.0861, -0.0395, -0.0612,\n",
       "                        0.0995, -0.0440, -0.0670, -0.0732,  0.0817, -0.0368,  0.0274,  0.0715,\n",
       "                        0.0863,  0.0624,  0.0871, -0.0686, -0.0115, -0.0917,  0.0925,  0.0796,\n",
       "                        0.0918, -0.0907,  0.0757, -0.0801, -0.0409,  0.0959, -0.0766, -0.0744,\n",
       "                       -0.0157,  0.0681, -0.0438, -0.0836, -0.0339, -0.0049,  0.0007, -0.0297,\n",
       "                       -0.0911, -0.0418,  0.0683,  0.0111,  0.0435, -0.0332, -0.0520,  0.0562,\n",
       "                       -0.0849,  0.0476,  0.0334,  0.0345, -0.0180,  0.0338, -0.0319, -0.0267,\n",
       "                       -0.0907, -0.0339, -0.0085,  0.0007,  0.0575, -0.0960,  0.0709, -0.0873,\n",
       "                       -0.1003,  0.0020,  0.0723,  0.0046, -0.0227, -0.0998, -0.0022, -0.0197,\n",
       "                        0.0421, -0.0185, -0.0524,  0.0616,  0.0416, -0.0188,  0.0202, -0.0085,\n",
       "                       -0.0860,  0.0844,  0.0855,  0.0443, -0.0574,  0.0514, -0.0143, -0.0649,\n",
       "                       -0.0558,  0.0464,  0.0096,  0.0102, -0.0377,  0.0732,  0.0355,  0.0267,\n",
       "                        0.0539, -0.0221, -0.0802, -0.0894, -0.0271, -0.0361, -0.0649, -0.0594,\n",
       "                        0.0561, -0.0596, -0.0206, -0.0568,  0.0111,  0.0163,  0.0335, -0.0099,\n",
       "                        0.0388, -0.0657,  0.0969, -0.0766, -0.0530, -0.0329,  0.0813, -0.0379,\n",
       "                       -0.0894, -0.0678,  0.0444, -0.0814, -0.0232,  0.0683, -0.0572,  0.0059,\n",
       "                       -0.0802, -0.0315, -0.0584,  0.0670,  0.0037,  0.0849,  0.0802, -0.0411])),\n",
       "              ('label.weight',\n",
       "               tensor([[-0.0756, -0.0047,  0.0031,  ...,  0.0346,  0.0726,  0.0652],\n",
       "                       [ 0.0303, -0.0176, -0.0581,  ..., -0.0842,  0.0937, -0.0309],\n",
       "                       [-0.0507,  0.0210, -0.0458,  ..., -0.0621,  0.0214,  0.0958],\n",
       "                       ...,\n",
       "                       [-0.0618, -0.0142, -0.0029,  ..., -0.0779, -0.0408,  0.0026],\n",
       "                       [ 0.0181,  0.0279, -0.0984,  ..., -0.0755,  0.0834,  0.0648],\n",
       "                       [-0.0121,  0.0302, -0.0676,  ..., -0.0525, -0.0312,  0.0540]])),\n",
       "              ('label.bias',\n",
       "               tensor([-0.0137, -0.0902, -0.0946, -0.0937,  0.0546, -0.0508,  0.0457, -0.0092,\n",
       "                       -0.0350,  0.0194, -0.0397,  0.0599, -0.0036, -0.0970, -0.0545, -0.0387,\n",
       "                       -0.0040,  0.0109, -0.0275, -0.0371, -0.0957]))]),\n",
       " 'model_opt': {'vocab_size': 24887,\n",
       "  'emb_dim': 100,\n",
       "  'emb_dropout': 0.3,\n",
       "  'emb_vectors': tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "          [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "          ...,\n",
       "          [ 0.3643,  0.1154, -0.0702,  ..., -0.3755,  0.8278, -0.0084],\n",
       "          [-0.1020,  0.7700,  0.1169,  ..., -0.1416, -0.1932, -0.4225],\n",
       "          [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       "  'lstm_dim': 100,\n",
       "  'lstm_n_layer': 1,\n",
       "  'lstm_dropout': 0.3,\n",
       "  'lstm_combine': 'add',\n",
       "  'n_linear': 1,\n",
       "  'linear_dropout': 0.5,\n",
       "  'n_classes': 21,\n",
       "  'crit': CrossEntropyLoss()},\n",
       " 'epoch': 0}"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['model', 'model_opt', 'epoch'])"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "odict_keys(['att_w', 'embedding_layer.weight', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'label.weight', 'label.bias'])"
      ]
     },
     "execution_count": 245,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['model'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'vocab_size': 24887,\n",
       " 'emb_dim': 100,\n",
       " 'emb_dropout': 0.3,\n",
       " 'emb_vectors': tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
       "         ...,\n",
       "         [ 0.3643,  0.1154, -0.0702,  ..., -0.3755,  0.8278, -0.0084],\n",
       "         [-0.1020,  0.7700,  0.1169,  ..., -0.1416, -0.1932, -0.4225],\n",
       "         [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " 'lstm_dim': 100,\n",
       " 'lstm_n_layer': 1,\n",
       " 'lstm_dropout': 0.3,\n",
       " 'lstm_combine': 'add',\n",
       " 'n_linear': 1,\n",
       " 'linear_dropout': 0.5,\n",
       " 'n_classes': 21,\n",
       " 'crit': CrossEntropyLoss()}"
      ]
     },
     "execution_count": 240,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "checkpoint['model_opt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMClassifier(**checkpoint['model_opt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding_layer): Embedding(24887, 100)\n",
       "  (embedding_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (lstm): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
       "  (lstm_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear_layers): ModuleList()\n",
       "  (linear_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (label): Linear(in_features=100, out_features=21, bias=True)\n",
       "  (crit): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(checkpoint['model'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (embedding_layer): Embedding(24887, 100)\n",
       "  (embedding_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (lstm): LSTM(100, 100, batch_first=True, bidirectional=True)\n",
       "  (lstm_dropout): Dropout(p=0.3, inplace=False)\n",
       "  (linear_layers): ModuleList()\n",
       "  (linear_dropout): Dropout(p=0.5, inplace=False)\n",
       "  (label): Linear(in_features=100, out_features=21, bias=True)\n",
       "  (crit): CrossEntropyLoss()\n",
       ")"
      ]
     },
     "execution_count": 247,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_sentence: 'The most common ENT_1_START audits ENT_1_END were about ENT_2_START waste ENT_2_END and recycling .'\n",
      "test_label: 'Product-Producer(e2,e1)'\n",
      "prediction: 'Message-Topic(e2,e1)'\n"
     ]
    }
   ],
   "source": [
    "predictor.pred_sent(dataset.INPUT)   # vscode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 代码梳理及细节回顾(在VScode中演示)\n",
    "\n",
    "　　在VScode环境中的训练文件里再回顾训练流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 作业\n",
    "  \n",
    "`【思考题】`思考这篇文章的模型有什么不足，有什么可以改进的地方，包括LSTM的部分以及attention的部分。\n",
    "\n",
    "`【代码实践】`复现该文章的模型部分代码。\n",
    "\n",
    "`【画图】`不看文章原图，按照自己的理解画出模型的结构图。\n",
    "\n",
    "`【总结】`对这篇文章进行回顾，思考并学习文章写作总体结构，模型设计等部分，并对相关工作进行总结（涉及使用RNN的工作）\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PYTH_1.6",
   "language": "python",
   "name": "pyth_1.6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
